%%! Created by: Kyle E. Davis
%   The Ohio State University
%   MailKyleDavis@gmail.com

%%! Document created using R's .rnw document editing, not LaTeX
%   platform (although both should work). UTF-8
%   Consider knitr, index, and bibliography when compiling.
%   Please read preamble, change file locations as neccesary.
%   Similarly, some data are called file a local file-path.

%%! All data are free and fair use, but please consider proper citation.
%   Some data have been collected via Google Big Query and TwittR;
%      These data have only partially been deidentified; please consider online
%      privacy when publishing/sharing data used here.


%------------ Quick Note on Coding Conventions -----------------

%  Primarily the intent for the document is in the .pdf output,
%  however, I have taken measures to add notes and organization
%  in the coding of this document, for my own use and for openess
%  to future authors.

%  %--- lines typically divide and add some organization
%  %!   usually indicates an important comment, or section
%  #    The same is true for notes in R code (##!) and (#---)

%----------------------- Preamble ------------------------------

\documentclass[12pt]{article}

% Margins:
\usepackage[top = 1in, left = 1in, right = 1in, bottom = 1in]{geometry}

% Holdover packages from long ago:
\usepackage{scrextend}
\usepackage{graphicx, fullpage, relsize, epsfig}    % tex tools
\usepackage{amsfonts, amsmath, amsthm, amssymb, mathtools, fdsymbol} % Math tools/ symbols

% Setting local graphics pathing:
\graphicspath{{C:/Users/Shing/Documents/GLM/Graduate_Methods_Handbook/figure/}}

% No idea:
\usepackage{siunitx}
\usepackage[compact]{titlesec}
\usepackage{enumitem}
\usepackage{textcomp} % Just for tilda's I think \sim?
\usepackage{verbatim} % No Idea

% For images and stuff, I think:
\usepackage{wrapfig}  % Wrap Figures and caption easier
\usepackage{setspace}
\usepackage[bookmarks = false, hidelinks]{hyperref} % Hides red box around links
\usepackage{booktabs}
\usepackage{subcaption}

%% Type Face/ Font Stuff    % fixed?
\usepackage[american]{babel}
% \usepackage{baskervald}
% \usepackage[T1]{fontenc}
% \usepackage[style = american]{csquotes}

%% using Todo Notes:
\usepackage{xargs}                      % Use more than one optional parameter in a new commands
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=small]{todonotes}
% Custom Todo Options:
\newcommandx{\prac}[2][1=]{\todo[linecolor=blue, backgroundcolor=blue!25, bordercolor=blue, #1]{#2}}
\newcommandx{\work}[2][1=]{\todo[linecolor=red,  backgroundcolor=red!25,  bordercolor=red,  #1]{#2}}

%% Making an Index:
\usepackage{makeidx}

%% Making a Bibliography:
\usepackage{natbib}
% Reference customization is included in the reference section.

%% Footnote Formatting
\setlist{nolistsep, noitemsep}
\setlength{\footnotesep}{1.2\baselineskip}
\deffootnote[.2in]{0in}{0em}{\normalsize\thefootnotemark.\enskip}

%% Section Formatting
\def\ci{\perp\!\!\!\perp}
\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}

%% Table of Contents Formatting (babel)
\addto\captionsamerican{ \renewcommand*\contentsname{Table of Contents:}}

%! Used in making graphs and plotting:
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{tkz-graph}
\usetikzlibrary{shapes,arrows}
\usepackage{caption}

% Custom Shortcuts:
\newcommand*{\h}{\hspace{5pt}} % for indentation
\newcommand*{\hh}{\h\h}        % double indentation

%! For tables:
\usepackage{dcolumn}
\usepackage{float}  % To use "H" in tables, place in location of LaTeX code
\usepackage{longtable}
\usepackage{array}

% for \thedate and other titling convieniences
\usepackage{titling}
\date{\today} % so that it's today (specified)

% -------------------------------------------------------------------
% ------------------------- Begin Document --------------------------
% -------------------------------------------------------------------
\makeindex
\begin{document}
%\graphicspath{Data Files}
\begin{flushleft}
\setlength{\parindent}{1cm} %1cm indent



\thispagestyle{empty}
\noindent \textbf{Introduction: Purpose and Goal}\\
\hfill \\
The purpose of this document is to provide an overview and tutorial for executing and critically interpreting various methods. A key contribution of this document is the ease to which you can swich between topics and search for conecpts, this was its inital purpose in helping me study for my minor exam.

Many of the notes come from Sean Gailmard' ``Statistical Modeling and Inference for Social Science,'' Skyler Cranmer's course in quantitative methods, and various other books on these topics including ``Event History Modeling, A Guide for Social Scientists'' by Janet M. Box-Steffensmeier and Bradford S. Jones.

The document here also follows a small dataset on beer rankings initially then covers data from the R Faraway package found here:

\begin{center}
\url{https://www.rdocumentation.org/packages/faraway/versions/1.0.7}
\end{center}

Code for visualizations is included often but not always feel free to email me for any code or data, including for \LaTeX. But ultimately this tutorial is less about replicating my results but to provide easily accessible tools for social scientists to use in their own research. That is, this document is only a first pass at a lot of these methods, to learn them better I would sugguest using them and failing over and over until something is learned. This is also not an exaustive list, I may reference something but never cover it later on. This only covers courses I've taken in my PhD program at Ohio State.

Code in this document will appear as follows:

<< setup, include=TRUE, cache=FALSE, echo=FALSE>>=
# Code Setup/ and Formatting:
library(knitr)
## This code is for figures, which may be similarly customized output visualizations:
## See https://github.com/yihui/knitr/blob/master/inst/examples/knitr-themes.Rnw  for more
## and https://github.com/yihui/knitr/blob/master/inst/misc/Sweavel.sty to fix arrow <_ issue vs <-
options(formatR.arrow=TRUE, width=78)

##! Changing out Theme to "kellys"
##! For Printing, consider changing this to "moe" or "edit-xcode"
##! Check out Themes at: http://animation.r-forge.r-project.org/knitr/
theme = knit_theme$get("kellys")
knit_theme$set(theme)
@


<<example code, warning=FALSE, message=FALSE, prompt=TRUE>>=
##!  Example Code:
X <- "String"
inverse_logit <- function(x){
     exp(x)/(1+exp(x))
}
@


\begin{center}
Created by Kyle Davis; davis.5211@osu.edu.\\ Summer 2017 --- \thedate ~(last updated)

For the most recent version:

\begin{center}
\url{https://github.com/KyleDavisGithub/Graduate_Methods_Handbook}
\end{center}

Any questions or concerns can be sent to me directly: davis.5211@osu.edu
\end{center}





\clearpage
%%! Page for TO Do Notes:
\listoftodos[For Review:]
\thispagestyle{empty}


\clearpage
%%! Page for Table of Contents:
\tableofcontents
\thispagestyle{empty}


%%! Clear page; set page counter at page "1"
\clearpage
\setcounter{page}{1}
%----------------------------------------------------------------------------------

\section{Causal Inference and Assumptions}

What is a \textit{cause}? More specifically, what in social science can really ``cause'' anything, is it ever really measurable? These are the questions that have been plaguing scientists for years, and with increases in methodology and theory we have tried to answer causal questions --- or at the very least, accurately report our assumptions and errors. A lot of the first few pages of this guide will be explicit in describing the assumptions we make with various methods, because these should be explicit for the audience when we find ``causal effects.''

Here we are using one language to define ``cause,'' specifically that used by social scientists, which is the first theoretical assumption. A cause here, in the simpilest terms that maximizes the utility of the word -- \textbf{is a treatment}. Anytime you see the word ``cause'' you could also say treatment, and the reverse. Therefore, race, gender, any attirbute is \textbf{not} a cause because we can not give people a new race or gender on a fundemental and societal level. But obviously these things matter! Here we would divide up our study, \textit{stratifying} on gender, race, etc for all of those groups which we cannot assign a new race or gender. Being creative in design up front can save a lot of our analysis on the back-end too; for example, creating an online program that generates a name or image for you on some online messenger program may give you a new \textit{perceieved race} or {perceieved gender} which we can randomize and identify. The key point here is that our definition of ``causal'' already introduces some problems, and a lot of what follows is attempting to advert them, understand them, and explain them accurately to the audience through \textit{causal inference}. Causal inference, then, is the way we infer causality through many design and analytical choices. One take-away here is that by broadening our language we strengthen the assumptions we can make, it's a give-and-take that has become convention. \index{cause}

\begin{longtable}{ll}
\hline\noalign{\smallskip}
\textbf{Symbol} & \textbf{Meaning} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
$D$ & Indicating Treatment Group ($1$) or Control Group ($0$)\\
$Y$ & Dependent Variable of Some Theoretical Interest        \\
$Y_0$ and $Y_1$ & Dependent Variable when Controlled or Treated\\
$X$ or $x_1$ etc. & Independent Variables\\
$V$ or $\sigma^2$ & Variance\\
E[~~]  & Expectation of Something, Typically the Mean\\
$E[Y |D]$ & Expectation of Y \textit{Given Some} D\\
$\mu_{ATE}$ & The Average Treatment Effect\\
$\mu_{ATT}$ & The Average Treatment of the Treated Group\\
$\mu_{ATC}$ & The Average Treatment of the Controll Group\\
$\xrightarrow[]{d}$ & The Distribution Becomes\\
$n$ or $N$ & The Number of Observations\\
$\mathcal{N}$ & Normally Distributed\\
$\hat{\theta}$ & Estimated Parameters\\
$X_{it}$ & All Rows Within X (i) and At Each Time (t)\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{longtable}


\clearpage
%-----------------------------------------------------------------------------------

\subsection{Pearl vs. Rubin Models of Causal Inference}
\hfill \\

Working with similar definitions of ``cause'' Rubin and Pearl are two scholars that have had different approaches to tackling causal inference. Both focus on weighted averages (means) to make the jump between data to the ``real world.'' Note that we could care about the distribution or variance instead of weighted means, but this won't be covered here. Rubin's approach is more algebraic, focusing on how attributes cannot be causal, and specifically stating when errors originate and cross between variables. Pearl leans on graphing theories, creating ``DAG'' plots (this will be covered later) to draw out for readers which variables influence others. Pearl's approach is intuitive and straightforward and seems to encapusalte theory just as well as Rubin's approach but has been criticized for not being explicit enough in error tracking between nodes of the theory plots. However, in some ways Pearl is more explicit algebraically, consider a fundemental mean difference in means between treatment and controlls:

\begin{equation}
\text{Rubin approach:} ~~~~ E[Y_1] - E[Y_0]
\end{equation}

\begin{equation}
\text{Pearl approach:} ~~~~ E[Y_1 | \text{do}(D=1)] - E[Y_0 | \text{do}(D=0)]
\end{equation}

Pearl's approach, using the \textit{do()} operator explicity states what the researcher \textit{did} and does not assume the reader has knowledge of what the researcher did in notation.


\hfill \\
\addcontentsline{toc}{subsubsection}{The DAG Plot}
\noindent \textbf{The DAG Plot:}
\hfill \\

% Here are some examples that should help:
\begin{center}
% The beginning tikzpicture [brackets] has the settings
% and functions for the rest of the graph, the graph will
% generate the nodes (the center connecting points) and
% the paths (the lines intersecting these nodes)

% more details on the measurements of these lines:
% https://tex.stackexchange.com/questions/8260/what-are-the-various-units-ex-em-in-pt-bp-dd-pc-expressed-in-mm
\begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=3cm,
    noname/.style={%
      rectangle,
      minimum width=2em,
      minimum height=2em,
      draw
    }
  ]
  % Nodes:
    \node[noname] (C)                                   {C};
    \node[noname] (D) [node distance=2cm, below right=of C] {D};
    \node[noname] (O) [right=of C]        				{O};
    \node[noname] (Y) [node distance=2cm, below right=of O] {Y};
  % Paths:
    \path (D) edge     node {} (Y)
          (C) edge     node {} (D)
          (C) edge     node {} (O)
          (O) edge     node {} (Y);
\end{tikzpicture}
\end{center}
\hfill

Consider our first plot (above), and imagine that it is some causal ``story'' we wish to convey to the reader. Here C influences both O and D which both influence Y. If we care about the effect of D on Y then we ought be careful that we are not measuring any effect of O on Y as well. This is the backbone to more complex DAG plots, or dynamic acyclic graphs. As a few definitions with these going forward, a \textbf{back door path} is any possible influence that goes into our cuasal variable rather than coming \textit{from} our causal variable to the dependent variable. Here C is a backdoor path, because Y could influence O (thus C) against the arrow, which in turn gives us false results if we merely measure D on Y. A \textbf{collider} is a node which has multiple variables running into it (in social science most variables will be colliders). Colliders hand-off information into these ``hubs'' which can greatly complicate our findings if not accounted for. Sometimes we may see broken lines or different shapes and subscripts to denote errors in these graphs (take THAT Rubin!) and how these are thought to exist between nodes. Note that DAG plots are made entirely from theory, and that qualitative and quantitative data alike can sculpt how we might model our causal phenomena of interest. Ultimately we care about a few things, (1) identifying exogenous variiation, (2) blocking backdoor paths, (3) sum up the front door paths. But how do we ``block'' these dangerous paths? Well there are many ways, the most common in experiments is to stratify the experiment by the node, but the \textbf{curse of dimensionality} tells us that as we do this more and more we will run out of responents (countries, etc) to be able to afford all of the strata we may want. Thus, many methods such as matching, and design choices help us account for blocking difficulties. \index{Curse of Dimensionality} \index{DAG} \index{Back-Door-Paths} \index{colliders}

Another great way to account for back-door-paths is to simply randomize, randomize everything in many different ways. By randomizing we shuffle who might get treated or not, with no interaction between them (SUTVA), and no selection into treatment. Although this will be mentioned in detail later. A final note is that sometimes researchers will put coefficents and effect sizes on the edges between nodes to show effects between and within variables of a study. Also when we see circles and dotted lines these are unobservable variables that are not measurable, or observable.


%----------------------------------------------------------------------
\begin{center}
\begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=2cm,
    noname/.style={%
      rectangle,
      minimum width=2em,
      minimum height=2em,
      draw
    }
  ]
  % Nodes:
    \node[noname] (A)                               {A};
    \node[noname] (u) [circle, node distance=2cm, right=of A]   {u};
    \node[noname] (B) [node distance= 1cm, above =of u]        	{B};
    \node[noname] (C) [right=of u]                  {C};
  % Paths:
    \path (A) edge     node [above] {} (B);
    \path (B) edge     node [above] {} (A);
    \path (B) edge     node [above] {} (C);
    \path (C) edge     node [above] {} (B);
    \path[dashed] (u) edge     node [left]  {} (A);
    \path[dashed] (u) edge     node [right] {} (C);
    \path[dashed] (A) edge     node [left]  {} (u);
    \path[dashed] (C) edge     node [right] {} (u);

    %\draw[->] (C) to [bend right] node [above] {?} (A);
\end{tikzpicture}
\end{center}
%------------------------------------------------------------------------





\subsection{Treatment (Causal) Effects, SUTVA, Confounding}

In its simplist form, we care about the difference between the average treated effect and the average control effect. The difference between the two is the \textbf{Naive Differences in Means}, or the naively simple calculation of treatment effects. Why is it naive? Well, we haven't considered any assumptions surrounding the groups, and the fundemental problem of causal inference --- the counterfactual. A counterfactual is the ``what if'' of being in the other group than the one assigned. \index{Naive Differences in Means}

\begin{equation}
\hat{\mu}_\text{Naive} = E[Y|D=1] - E[Y|D=0]
\end{equation}


Think for a second, given all of this, when might the $\hat{\mu}_\text{Naive} = \hat{\mu}_\text{ate}$? Or, when might the naive estimator be the same as our average treatment effect (ATE)? Remember that the reason why $\mu_{naive}$ is naive is because it never oberserves the counterfactual world.
\prac[inline]{Think: Causation, When will the ATE be the same as the Naive Estimator?}

\hfill \\

\noindent \textit{Answer:}\\

Note that the above equation has no decorations on $Y$, we are assuming that all respondents are not interacting with each other, even over time, (SUTVA) and that our design is absolutely complete (think of a complete DAG plot) such that we really are just getting $ E[Y_1] - E[Y_0]$. So no error, and no SUTVA violation we get the $ate$ to equal $\mu_{Naive}$. \index{SUTVA} Here SUTVA is doing a lot of work, we are assuming with SUTVA that there is no interference and treatment variation ($ Y = Y_1)$. This is a pretty monsterous assumption, countries, people, politicians, the media, \textit{everyone} often communicates and shares what they have been ``treated'' with all the time! Beyond this, treatments often effect people very differently depending on affiliaiton, openness to feedback, or if you ate breakfast that morning. Some methods like network analysis will embrace this fact instead of avoiding, although this method will not be covered here.

\hfill \\

\noindent SUTVA, or the Stable Unit Treatment Value Assumption, follows two primary tennants:

\hfill \\

\begin{itemize}
\item[1.] That the treated and controlled units are not interacting between one another.
\item[2.] That the method of how someone gets treated does not varry between units, or is at least eqivelent.
\end{itemize}

\hfill \\

\noindent Some core assumptions are not only SUTVA (consistency) but also that no \textit{confounding} occurs (also known as selection on observables, ignorability). This is when the people that were to benefit most from the treatment select into the treatment. No confounding formally is: \index{confounding} \index{selection on observables} \index{ignorability}

\begin{equation}
D \Vbar Y_1, Y_0 = P(D=1 ~ | ~ Y_1, Y_0)
\end{equation}

This language, by the way, is really annoying because: selection on observables $=$ conditional independence $=$ conditional ignorability $=$ no confounding, conditional on covariates $=$ blocking all back door paths. \index{blocking} \index{selection on observables} \index{back door paths} Open backdoor paths, is Pearlsian DAG-speak for having confounding. If we have more treated folk for each $Y_0$'s than $Y_1$'s then we have imballance. If X is correlated with $Y_1$ and $Y_0$ for \textit{any reason}, then even though we randomized, then $Y_1$ and $Y_0$ would be informative about $X$ - And therefore $Y_1$ and $Y_0$ would be informative about our treatment D! Therefore we have a broken experiment.

Some other things to look out for is the plug-in estimator and conditional expectation functions, each of these are just examining average treatment effects around conditions (control variables). The \textit{plug-in estimator} is when we condition on and control for the average treatment effect (ATE) summing up over our controlled $X$'s.

\begin{equation}
E_X[E[Y |  D = 1, X] − E[Y |D = 0,X]]
\end{equation}

\noindent The conditional expectation function (CEF) is when we take the average treatment effect to see conditional causal effects.

\begin{equation}
E[Y |D] = E[Y_0] + \mu_{ate}D
\end{equation}

We could think about taking into account all variables and possible conditions, this is the hope of Rubin's \textit{saturated model}, which has it's roots in imputation for trying to find missing data (the missing data here in our case would be the counterfactuals). Saturated models do come with particular downsides however, mostly in the errors it produces, and our lack of knowing the exact causal pathway given that adding new variables can open up back-door-paths to our causal estimand. \index{saturated modeling} \index{imputation}


%--------------------------------------------------------------------------------------------------------------
\hfill \\

\subsection{Identification Through Randomization, Hypothesis Testing}

Identification strategies are the attempt to find the missing data that we don't know due to the fundemental problem of causal inference. Two popular paths to do this are using controls and randomization.

There are different types of randomization to help us gain inference. Simple randomization is a probability of treatment per individual ($D$ ``treatment'' being applied) in all possible cases; complete randomization is deciding what fraction of observations we need to fulfil for our total group and then curate our probability of treatment to this. Using randomization, we can hope to ``block'' on these observations (like gender, or X) which we cannot randomally assign.

\begin{equation}
D \Vbar Y_1, Y_0 | X ~ \text{or,} ~ P(D=1 ~ | ~ Y_1, Y_0; X) = p
\end{equation}

\hfill \\

Essentially, we could calculate the naive differences in means for each possible estimation of all of treatment and control groups: and that is called randomization inference (Rubin's causal model).


\hfill \\

In hypothesis testing we could just take with and without treatment and see how our distributions converge (remember the t-test stuff)

\begin{equation}
\sqrt{n}(\hat{\mu} - \mu) \xrightarrow[]{d} \mathcal{N}(0, \sigma^2)
\end{equation}

\index{p-values}
Cool, what's the p-values that we get from this? It is saying that conditional on the null hypothesis (i.e., conditional on $\mu_{ate} = 0$ ), what is the (frequentist) probability of observing this extreme an estimate of $\hat{\mu}$? This is typically in regards to a sharp null hypothesis of zero average effects, $Y_1 = Y_0$ always. \index{sharp null hypothesis}

\prac[inline]{Think: What really is a p-value telling us?}


%------------------------------------------------------------------------------------------

\hfill \\


% Slide 45 of causal 4 has regression components.

%------------------------------------------------------------------------------------------
%    Law of itterated expecations? it is when we take the E[E(Y|D)] we can boil it down to E[Y]
%
Note that our experiments can lose observations rather quickly not only through design but in human execution (for various reasons). Consider the hypothetical flow chart on the following page for a good way to report these losses to readers (often included in supplemental appendix online). This has the added visual benefit of informing readers (reviewers) of your paper's design,


% A CONSORT-style flowchart of a randomized controlled trial
% using the PGF/TikZ package
% Author  : Morten Vejs Willert (July 2010)
% License : Creative Commons attribution license
\newpage
\begin{center}
  % setting the typeface to sans serif and the font size to small
  % the scope local to the environment
  \sffamily
  \footnotesize
  \begin{tikzpicture}[auto,
    %decision/.style={diamond, draw=black, thick, fill=white,
    %text width=8em, text badly centered,
    %inner sep=1pt, font=\sffamily\small},
    block_center/.style ={rectangle, draw=black, thick, fill=white,
      text width=8em, text centered,
      minimum height=4em},
    block_left/.style ={rectangle, draw=black, thick, fill=white,
      text width=16em, text ragged, minimum height=4em, inner sep=6pt},
    block_noborder/.style ={rectangle, draw=none, thick, fill=none,
      text width=18em, text centered, minimum height=1em},
    block_assign/.style ={rectangle, draw=black, thick, fill=white,
      text width=18em, text ragged, minimum height=3em, inner sep=6pt},
    block_lost/.style ={rectangle, draw=black, thick, fill=white,
      text width=16em, text ragged, minimum height=3em, inner sep=6pt},
      line/.style ={draw, thick, -latex', shorten >=0pt}]
    % outlining the flowchart using the PGF/TikZ matrix funtion
    \matrix [column sep=5mm,row sep=3mm] {
      % enrollment - row 1
      \node [block_center] (referred) {Referred (n=173)};
      & \node [block_left] (excluded1) {Excluded (n=17): \\
        a) Did not wish to participate (n=9) \\
        b) Did not show for interview (n=5) \\
        c) Other reasons (n=3)}; \\
      % enrollment - row 2
      \node [block_center] (assessment) {Assessed for eligibility (n=156)};
      & \node [block_left] (excluded2) {Excluded (n=54): \\
        a) Inclusion criteria not met (n=22) \\
        b) Exclusion criteria(s) met (n=13) \\
        c) Not suited for group (n=7) \\
        d) Not suited for CBT (n=2) \\
        e) Sought other treatment (n=3) \\
        f) Other reasons (n=7)}; \\
      % enrollment - row 3
      \node [block_center] (random) {Randomised (n=102)};
      & \\
      % follow-up - row 4
      \node [block_noborder] (i) {Intervention group};
      & \node [block_noborder] (wlc) {Wait-list control group}; \\
      % follow-up - row 5
      \node [block_assign] (i_T0) {Allocated to intervention (n=51): \\
      \h Received intervention (n=49) \\
      \h Did not receive intervention (n=2, \\
      \hh 1 with primary anxiety disorder, \\
      \hh 1 could not find time to participate)};
	  & \node [block_assign] (wlc_T0) {Allocated to wait-list (n=51): \\
      \h Stayed on wait-list (n=48) \\
      \h Did not stay on wait-list (n=3, \\
      \hh 2 changed jobs and lost motivation, \\
      \hh 1 was offered treatment elsewhere)}; \\
      % follow-up - row 6
      \node [block_lost] (i_T3) {Post-intervention measurement: \\
      \h Lost to follow-up (n=5, \\
      \hh 2 dropped out of the intervention, \\
      \hh 3 did not complete measurement)};
	  & \node [block_lost] (wlc_T3) {Post-wait-list measurement: \\
      \h Lost to follow-up (n=6, \\
      \hh 3 dropped out of the wait-list, \\
      \hh 3 did not complete measurement)}; \\
      % follow-up - row 7
      % empty first column for intervention group
      & \node [block_assign] (wlc_T36) {Allocated to intervention (n=48): \\
      \h Received intervention (n=46) \\
      \h Did not receive intervention (n=2, \\
      \hh 1 reported low motivation, \\
      \hh 1 could not find time to participate)}; \\
      % follow-up - row 8
      \node [block_lost] (i_T6) {3-months follow-up measurement: \\
      \h Lost to follow-up (n=9, \\
      \hh did not complete measurement)};
      & \node [block_lost] (wlc_T6) {Post-intervention measurement: \\
      \h Lost to follow-up (n=5, \\
      \hh 2 dropped out of the intervention, \\
      \hh 3 did not complete measurement)}; \\
      % follow-up - row 9
      % empty first column for intervention group
      & \node [block_lost] (wlc_T9) {3-months follow-up measurement \\
      \h Lost to follow-up (n=2, \\
      \hh did not complete measurement)}; \\
      % analysis - row 10
      \node [block_assign] (i_ana) {Analysed (n=51)};
      & \node [block_assign] (wlc_ana) {Analysed (n=51)}; \\
    };% end matrix
    % connecting nodes with paths
    \begin{scope}[every path/.style=line]
      % paths for enrollemnt rows
      \path (referred)   -- (excluded1);
      \path (referred)   -- (assessment);
      \path (assessment) -- (excluded2);
      \path (assessment) -- (random);
      \path (random)     -- (i);
      \path (random)     -| (wlc);
      % paths for i-group follow-up rows
      \path (i)          -- (i_T0);
      \path (i_T0)       -- (i_T3);
      \path (i_T3)       -- (i_T6);
      \path (i_T6)       -- (i_ana);
      % paths for wlc-group follow-up rows
      \path (wlc)        -- (wlc_T0);
      \path (wlc_T0)     -- (wlc_T3);
      \path (wlc_T3)     -- (wlc_T36);
      \path (wlc_T36)    -- (wlc_T6);
      \path (wlc_T6)     -- (wlc_T9);
      \path (wlc_T9)     -- (wlc_ana);
    \end{scope}
  \end{tikzpicture}
\end{center}

\newpage

\noindent On hypothesis testing, in most cases we start off with testing against a \textit{sharp null hypothesis of zero average effects}. That is, $\mu_{ate} = 0$. Note that this is probably the most boring, unrealistic, and uninteresting null hypothesis that we could ever test against. Does anything, in social science, really ever have a sharp zero effect? Probably not, but we use our handy friends, the z-test and t-test, to find the statistical significance of our finding compared to the zero effect. For small samples we often assume that, because of the large sample size used, the unkown population variance may be replaced by the sample variance. That is, instead of a z score we use a t score:

\begin{equation}
z= \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}
\end{equation}

\noindent one would use:

\begin{equation}
t = \frac{\bar{x} - \mu}{s_x / \sqrt{n}}
\end{equation}





\subsection{Frequentism and Bayesianism: What are we really doing here?}

``Statistics'' has its roots, interestingly, in social sciences as the origins of the word derives from ``state-istics'' or the study of the state. This methodology, uniquely bred for States to understand it's citizenry, statistics have become fascinatingly turned for the \textit{people} to now understand their State. Following World War II this was more important than ever, a social science with deep philosophical, theoretical, and qualitative research began to explode with quantiative methodology in through the 1950's. These methodologies, deeply aided by fields like biology, computer science, and econometrics, have both drawn us closer to understanding public and governmental behavior \textit{and} has driven us farther from these concepts as methodology has been misused (e.g. p-hacking), misunderstood, and miscommunicated.

To begin to understand why we use statistics, general linear models, or machine learning, one must first understand a few basic underpinnings of frequentist theory versus Bayesian theory. Frequentism comes in social science largely from R.A. Fisher's idea of likelihood theory of inference. It argues that one true parameter ($\theta$) exists in nature and all scientists are doing are getting multiple samples to guess the one-true-fixed $\theta$. Our results are likelihoods that are close to measuring the one-true DGP but \textbf{always} with some uncertainty. In theory, we can map our estimated DGP $\hat{\theta}$ in two-dimensions pretty easily (think of a basic x--y coordinate grid). Yet, the social world is very complex, as we add more and more variables we begin to get to a point where we find ourselves in hyperspace, unable to map and visualize our surroundings (thus, machine learning helps here, but this will come up later). To help find interesting points when we do these regressions, we (in frequentist theory) seek the MLE or the Maximum Likelihood Estimation. Note this is purposefully not the Maximum \textit{Probabilistic} Estimation, in likelihoodist theory these words mean very different things. The take-away here should be to be careful when speaking in likelihoods (a relative measure of uncertainty) versus probabilities (results only from randomized experiments) as these words have strict definitions in causal inference.

An MLE is interesting because it is the impasse between our estimated parameter $\hat{\theta}$ and our $y$ dependent variable. This typically occurs at the ``highest'' or maximum point on whatever curve we have. Consider the image below, the red line indicates the plateau or maximum point, for the liklihood ($L$) of $y$ (estimated by $x_1$, etc.) on our $\theta$.

\begin{center}
\includegraphics[scale=2]{MLEex}
\end{center}

\clearpage

To find, mathematically, our MLE (which occurs under-the-hood in R) we compute the following: \index{MLE}

\begin{list}{}{}
\item[1.] We $\log$ the likelihood function for ease of computation.
\item[2.] We then take the derivative (also known as the score-function).
\item[3.] Then set this derivative equal to zero (to find our flat top part).
\item[4.] we then solve for $\hat{\theta}$.
\item[5.] We then make sure found the maximum and not some minimum peak (note we could have found the other peak in the above image). To do this we check the sign of the second derivative, if negative we know the curve is downward-facing. We finish by calculating the Fisher information (the second derivative) to find our confidence via variance and standard errors.
\end{list}





\subsection{Running Assumptions}

\prac[inline]{Think: What assumptions are in causal analysis?}

Typically we trade-off assumptions but can never completely eliminate them, only show or \textit{prove} that we have fulfilled them -- all to get closer to causation (causal inference). Some running assumptions are:

\begin{itemize}
\item[1.] SUTVA
\item[2.] No confounding
\item[3.] Probability exists only between 0 and 1, but never zero nor 1.
\end{itemize}

\noindent If we attempt to "lean-into" one of these assumptions to be able to measure it, we may come across additional assumptions like "parrallel paths" or various representation and error assumptions. Some of these will be discussed in the Regression section of this book.


\subsection{Non-parametric Modeling}

A few non-parametric models help us lean away from certain assumptions. This could be an entire class on it's own, but I have a few notes that I'll share here.


\subsubsection{Multiple Comparisons}


\subsubsection{Marginal Structural Models (MSM)}
\index{Marginal Structural Modeling}

If we are working with relational datasets, and do have downstream effects at $t_1$, $t_2$, $t_n$ we cannot solve this problem by using fixed effects. Because we are assuming with fixed effects that our estimands do not change over time, this is actually a core assumption that is violated all the time. However, fear not, for marginal structural models can help us when fixed effects cannot.

Assuming time doesn't influence previous times (which may be an issue if we're studying some forecasting or strategic process that appears in our data). we can look at the outcome in th elast time period and stack each time period on top of one another. This is very different from adding a bunch of dummy variables, it is stacking the time periods together; we can treat for the histories in this way and potential influence amongst them.\footnote{It may be a good idea to check out time series analysis here, just in case you problems can be solved with your data.}

So marginal structural models can work with different treatment histories (arms) than fixed effects. Let's fit a model for every ``D,'' and just weight each of these down the road. We conditionon the history of past events, and if the model pukes at us then we drop the bad cases and redefine the subsample we are talking about, and how it's still interesting.

Note, we do not get out of the unobservables problem, we still need to set up this model with the same theory and careful design. \textit{You cannot analyize your way out of a bad design. }

Some code to look into, (and notes):

<<msm1, prompt=TRUE, eval=FALSE>>=
# GLM has an argument called "weights," don't trust it...

library(survey) # this is better for a variety of reasons.
     svydesign(ids = , weights = ~1, data)
     svyglm( Y ~ D, unweighted_design, family = quasibinomial)
     coef(The_Model)

@

These weighted coefficents can help us, but we can try to stabilize teh weights we get, especially in cases of low probability events. For this we need non-dynamic probabilities. Assign these values with R's ``mgcv'' package, and function ``bam'' and some smoothing with \texttt{s()}. Get the t-specific weights, and there are ways to plot the improvements  from doing this. Then, put the weights together (stabilized) so mean aroudnd 1, and multiply these easily with \texttt{prod(weights)}. Change out these weights in above, instead of \texttt{1} we can use \texttt{ weights}:

<<msm2, prompt=TRUE, eval=FALSE>>=

# after checking out:
library(mgcv)
     bam()
     s()

# Then we can edit things:
library(survey) # this is better for a variety of reasons.
     svydesign(ids = , weights = ~weights, data)
     svyglm( Y ~ D, weighted_design, family = quasibinomial)
     coef(The_Model)

     # Might wan to check out package: cbmsm
@

We can then get predicted probabilities from our marginal structural models. So essentially with marginal structural models:

\begin{itemize}
\item[1.] Set up data so each outcome has multiple individuals and prior time periods.
\item[2.] Model treatment in each period without tie-varying covariates with them.
\item[3.] Asses ballance (repeat 2 if necessary).
\item[4.] stabilize weights.
\item[5.] calculate weighted estimates of QOI's.
\item[6.] What is N anymore? Let's just always bootstrap the heck out of this: doing (2), (4), (5), over 1,000 times over.
\end{itemize}

\hfill \\
\hfill \\

%------------------------------------------------------------

\subsection{Finding Causal Estimands:}

\hfill \\

\subsubsection{Matching and Propensity Scores:}

\textbf{Matching is weighting} each observation to all strata. Much of this is really the same thing, matching is pairing individuals and comparing causal estimands and weighting is forcefully multiplying a ``weight'' onto our variables to pair them to reality (say the Census). Subclassifcation is the same thing, in the long, as matching and weighting. If we don't have exact matches we can use a coarse matching approach where we break up a continuous variable into deciles. This is similar to K-Nearest Neighbor subclassification matching. \index{matching} And a propensity score is just the probability of treatment given some X. \index{propensity scores}

\begin{equation}
e(X) = Pr(D = 1|X)
\end{equation}

The idea is then to coarsen/subclassify or nearest-neighbor match on the \textbf{propensity score}. \index{coarse matching} \index{nearest-neighbor} \textbf{Subclassification} is making the probabilities of treatment and controll equivilant in observational studies. Doing this, we help create ballance in our treated and controlled groups. This is similar to complete randomization in experimental trials. (What machine learning techniques can help us here?) \index{subclassification}

Some machine learning approaches that can help us here (see machine learning chapter for more) include regression trees and other classification approaches. These can help us identify groups to match on create more ballance in our data.




\hfill \\

\subsubsection{Fixed Effects:}

Fixed effects are dummy varriables (are stratification) and are turning ``on or off'' a year, country, or some other theoretically interesting variable to ``account'' for the things that pour into a time or place. Using this we assume no carry over between years to previous or future years. Here are the assumptions with fixed effects: \index{Fixed Effects} \index{Marginal Structural Models}

\hfill \\

\begin{itemize}
\item[1.] no unobserved time-varying confounder exists
\item[2.] past outcomes do not directly affect current outcome
\item[3.] past outcomes do not directly affect current treatment
\item[4.] past treatments do not directly affect current outcome
\end{itemize}

\hfill \\

\noindent One larger problem is that the linear fixed effect does not consistently estimate the ATE:

\begin{equation}
\beta_{LFE} \rightarrow \frac{E[V^i \mu^i_{naive}]}{E[V^i]} \neq \mu_{ate}
\end{equation}

\noindent But marginal structural models allow for our times and events to influence one another (more on that later).


\hfill \\

\subsubsection{Differences in Differences}
\index{Differences in Differences}
Differences in Differences is a way to estimate our causal estimand (ATT, ATE, etc) and map it when treatment rolls out. Say we have three time points 1, 2, and 3, where all are untreated at 1, some are treated at 2, and all are treated at 3. To get the estimand from differences in differences we first subtract (getting the difference) of the mean outcomes between times and 2 and time 1 for the treated and controlled then calculate the differences in those two groups between the two time groups. We could do the same thing for 3 and 2. An assumption for difference in differences designs is that things were to be on the same parallel paths, or that we need to prove to the reader (often through theory) that the treated and controlled group -- in a world where no treatment occured, actually would've been the same (parrallel to one another). Further, that it is only our treatment group that caused a change between the control and treated group, not anything that would've happened anyways.

\hfill \\

\subsubsection{Instrumental Variables}

Here imagine we are interested in the causal effect of D --> Y but our errors, or some unknown (u) covariate influences both our treatment and outcome. What we can do is add in some "garbage" variable (Z) into the mix, and see just how Z passes through to Y. It helps me think of "pipes" where we want to see how much [insert liquid here] passes through D to Y, neverminding covariate u.

\begin{center}
\begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=3cm,
    noname/.style={%
      rectangle,
      minimum width=2em,
      minimum height=2em,
      draw
    }
  ]
  % Nodes:
    \node[noname] (Z)                                   {Z};
    \node[noname] (D) [node distance=2cm, right=of Z] {D};
    \node[noname] (Y) [right=of D]        				{Y};
    \node[noname] (u) [circle, node distance=2cm, above right=of D] {u};
  % Paths:
    \path (D) edge     node {} (Y)
          (Z) edge     node {} (D)
          (u) edge     node {} (D)
          (u) edge     node {} (Y);
\end{tikzpicture}
\end{center}
\hfill
















%----------------------------------------------------------------------------------
\clearpage
\section{Survey Design}
\hfill \\

\subsection{The Survey Experiment}

\subsection{Types of Questions}

\subsection{Psychology of Design}



\hfill \\
\subsubsection{The "Don't Know" Option:}



\subsection{Useful Rules of Thumb}

\subsection{Research on Survey Biases}


\clearpage
%-----------------------------------------------------------------------------------










\clearpage
\section{Regression:}

\begin{equation}
\mathlarger{ y = \beta_0 + \beta X + \epsilon }
\end{equation}

The General Linear Model (GLM) consists of a \textit{stochastic and systematic} components. The stochastic component is the section in the equation which changes (``randomly'') and has error our error term ($\epsilon$), whereas the systematic component remains constant with data values ($X$) and is used to test our dependent variable ($y$) and give us interpretive values in the form of coefficients ($\beta$). To ``generalize'' this to other models (as we'll see in this document) we need a distribution and a link function, which will come up later.

It should be mentioned that this GLM formula has it's genesis in basic geometry's equation for a line $y=mx + b$ the root of this should be intuitive as we attempt to fit a line to our data and estimate it using maximum likelihood.

No matter what the model we're specifying throughout this document, be it logit, probit, Poisson, or survival models the linear predictor will always be the same:

\begin{equation}
\mathlarger{ \eta = X\beta \text{ \text{  }  or,  }}
\mathlarger{ \eta_i = \beta_0 + \beta_1 x_{i1} \textbf{...} \beta_q x_{iq}  }
\end{equation}
\prac{Set up a problem here}
One example of when we might use regression is when trying to determine the best quality of beer based upon mean judged ratings, these are the questions that matter the most. Beer ratings range from $-1$: ``Fair'', to $0$: ``Good'', and $1$: ``Great.'' Consider our first model where we consider how expensive the beer is:

\begin{equation}
\mathlarger{ \text{Beer Rating} = \beta_0 + \beta \text{Beer Price} + \epsilon }
\end{equation}

\noindent Given the information from our data regressed in R: (This is shown on the following page)

\begin{equation}
\mathlarger{ \text{Beer Rating} =  (-0.76) + (.26) \text{Beer Price} + \text{error} }
\end{equation}

We can solve this to find our beer rating with the substitution of a beer price, we remove the error term because we assume it's normally distributed and centered at zero (an assumption that will come up in the following section). Let's assume we are going out with friends and only have \$2.00:

\begin{equation}
\mathlarger{  \text{Beer Rating} = (-0.76)+(.26)\cdot 2.00 = -.24 }
\end{equation}

This means, given that our average (``Good'') beer rating is zero, that if we only had two dollars to spend on a beer we would get a slightly below average beer ($-.24$). This is assuming the price of beer and rating of beer is a linear relationship, this isn't an assumption we always have to make but one we will for now.

\clearpage
\noindent Showing this in R, let's load and set up our data and run the model that we used to get the numbers above: \index{loading data} \index{recoding data}
<<BeerData, warning=FALSE, prompt=TRUE>>=
## Initial pass: reading data, looking at variables.
# Data available, email: MailKyleDavis@gmail.com

library(foreign) # For reading data.
beer_data <- read.dta('C:/Users/Shing/Documents/Previous Course Info/Quant II/beer.dta')

library(car) # for ease of recoding variables
# Our Beer ratings could be coded more intuitively:
beer_data$rating <- as.numeric(beer_data$rating)
head(beer_data) # Check out the variables we have available to us:
nrow(beer_data) # For 35 beers:

# Let's recode our rating dependent variable to be more intuitive:
# reassign values: where 3 is now 0, etc..
beer_data$rating <- recode(beer_data$rating,"3=0; 2=1; 1=2")
class(beer_data$rating) # numeric and coded intuitively, could be better...
# From -1 to 1
beer_data$rating <- recode(beer_data$rating,"0=-1")
beer_data$rating <- recode(beer_data$rating,"1=0")
beer_data$rating <- recode(beer_data$rating,"2=1")

beer_data$rating # Setting median to zero will help our interpretation

@


<<FirstModel, prompt=TRUE>>=
set.seed(12345) # Setting our seed will fix the randomness for replication

#    Let's do the basic linear model thing:
##!  lm(Y ~ X, data selection)
mod1 <- lm(rating ~ price, data = beer_data)
# just list our coefficients (to see everything use summary(mod1))
mod1$coefficients
@


<<Texreg1, eval=FALSE, echo=TRUE, warning=FALSE, message=FALSE, prompt=TRUE, highlight=TRUE >>=
# Texreg Tables:
library(texreg)

texreg(l= list(mod1), stars = numeric(0),
       custom.coef.names    = c("Intercept", "Beer Price"),
       caption.above        = T, float.pos = "h!",
       custom.note          = "Dependent variable: Beer Rating")
@


\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c }
\hline
 & Model 1 \\
\hline
Intercept  & $-0.76$  \\
           & $(0.36)$ \\
Beer Price & $0.26$   \\
           & $(0.11)$ \\
\hline
R$^2$      & 0.14     \\
Adj. R$^2$ & 0.11     \\
Num. obs.  & 35       \\
RMSE       & 0.74     \\
\hline
\multicolumn{2}{l}{\scriptsize{Dependent variable: Beer Rating}}
\end{tabular}
\end{center}
\end{table}

\index{texreg package}


The above table shows the standard errors (in parentheses) being two standard errors away from our coefficients, therefore being significant at the 95\% confidence level. We have 35 observations and our $\text{R}^2$ explains 14\% of the variation in our dependent variable. The adjusted $\text{R}^2$ accounts for the amount of variables added into a model and is slightly more robust, prevent against ``kitchen sink'' models which add many variables to get a high  $\text{R}^2$. It explains 11\% of the variation.
\index{R$^2$}

Our intercept is when all else is held at zero ($\beta_0$) so the most average priced beer has a rating of $-.76$, which is overall pretty poor quality beer. Yet, if we spend one dollar more, our ranking jumps by .26 (a quarter rank) each time. This can be visually represented alongside simulations in a plot here: \\
\index{Simulation}

\index{base-R plotting}
\begin{center}
<<simulation1, out.width='4in', warning=FALSE, message=FALSE, prompt=TRUE>>=
library(arm) # For sim()
# Let's plot a simulation:
plot(beer_data$price, jitter(beer_data$rating), pch=19,
     xlab= "Price",
     ylab= "Rating",
     main= "Rating by Price of Beer: with 100 simulations")
abline(mod1)  # Add our model line
# Simulate 100 possible outcomes, take their lines, plot them on top
m1sim <- sim(mod1)
for (i in 1:100){
  curve (coef(m1sim)[i,1] + coef(m1sim)[i,2]*x, add=T, col="grey")
}
curve (mod1$coef[1] + mod1$coef[2]*x, add=T, lwd=3)
@
\end{center}

The grey lines here are simulations which display our confidence intervals around the black regression line for our model. They occupy a low amount of the variation in our dots so intuitively we can see how this would yield a low $\text{R}^2$ (our model only had $.14$). See below:

<<lineartwo, eval=FALSE, prompt=TRUE>>=
mod2 <- lm(d$rating ~ d$price + d$alcohol)

texreg(l= list(mod1, mod2), stars = numeric(0),
       custom.coef.names = c("Intercept", "Beer Price", "Alcohol Content"),
       caption.above = T, float.pos = "h!",
       custom.note = "Dependent variable: Beer Rating")
@


\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Model 1 & Model 2 \\
\hline
Intercept       & $-0.76$  & $-2.89$  \\
                & $(0.36)$ & $(0.91)$ \\
Beer Price      & $0.26$   & $0.21$   \\
                & $(0.11)$ & $(0.11)$ \\
Alcohol Content &          & $0.50$   \\
                &          & $(0.20)$ \\
\hline
R$^2$           & 0.14     & 0.28     \\
Adj. R$^2$      & 0.11     & 0.23     \\
Num. obs.       & 35       & 35       \\
RMSE            & 0.74     & 0.69     \\
\hline
\multicolumn{3}{l}{\scriptsize{Dependent variable: Beer Rating}}
\end{tabular}
\end{center}
\end{table}

We could consider a model where alcohol content is additionally important, Model 2. Here our R$^2$ increases to .28; and our variables remain within two deviations of the error, with the slight exception of beer price. Our intercept loses it's interpretation, we are now looking at where alcohol content is at zero. Our beer rank jumps by .50 every one increase in alcohol content. Both of these effects are controlling for the other.












%----------------------------------------------------------------------------------------------

\clearpage
\subsection{Modeling Assumptions, Diagnostics}
\hfill \\
In social science empirical research there is no dearth of modeling errors. The following diagnostics and error-checking may not seem worthwhile because this work rarely makes it into the actual publication; but rather into a supplemental appendix. This work too helps a lot to ease skeptical readers about the claims we make (helping us get published).

\begin{equation}
\mathlarger{  \epsilon_i  \thicksim \mathcal{N}(0, \sigma^2)             }
\end{equation}
\noindent \textit{Errors are Normally Distributed}\\
We assume that the errors are normally distributed, to check for this histograms can give us a first pass, but one can also map the residuals and (hopefully) see their ``random'' distribution with no foreseeable trends. the \texttt{plot(model)} command in R can give a few of these plots in a row pretty easily too. Consider our beer rankings model from earlier: \\
\hfill \\

\begin{center}
<<checkinglinear, out.width='5in', prompt=TRUE>>=
par(mfrow=c(2,2)) # plot multiple alongside each other:
plot(mod1)        # plot our quick model checks
par(mfrow=c(1,1)) # Reset plotting option
@
\end{center}
\index{checking linear models}

\noindent \textit{Bias in Error}\\
Next, we assume that there is no bias in our errors, meaning that our expectation of our errors in zero: $\text{E}(\epsilon_i) = 0$. It is rare to get exactly zero, but we should report exactly what our errors are and be sure to mention any outliers, leverage points, or otherwise influential points to the reader so they are not mislead. We can handle these situations in various ways:\\
 \textbf{Never outright delete data points.} Theoretically, the social world is vastly complex and rarities can (and do) exist in nature. If we have an influential point, where our data point(s) are not necessarily malicious or supremely rare, but rather pull our analysis in one way or another we can transform all of the data points (log or rescale the variable) to decrease the influence of one point. Observing points outside of ``Cook's Distance'' (in plot R command) can be a good way to observe influential points or outliers. \\
\hfill \\

\noindent \textit{Uniform Variance}\\
Our $\sigma^2$ is not subscripted, meaning we expect a uniform variance in our error term, where there is no increasing or decreasing in our errors. homoscedasticity here is a good thing, we can detect this by plotting our residuals and seeing random variance of our points across the mid-point. We do not want to see heteroscedasticity or where errors funnel outward or have a certain trend in themselves.

\hfill \\
\noindent \textit{Autocorrelation, iid}\\
We do not want our observations to correlate with one-another, meaning our individuals or observations are not influencing another individual respondent's results. Each data point should be independent draws from one another.

\hfill \\
\noindent \textit{Measurement Error (X)}\\
We need to make sure the phenomenon we wish to measure is actually being measured by our data. A question such as ``What do you think about healthcare'' can be responded by a battery of cognitive processes; but a question like ``To what degree of anger do you feel about the current healthcare policy?'' is far more precise to the theoretical process we care to study. Beyond theory, the scale of our variable and our operationalization can be changed and shifted to answer different questions. In Section 3 that follows, for example, we will dichotomize a variable to observe a different question about how we think about our beer rankings.

\clearpage
\hfill \\
\noindent \textit{Omitted Variable Bias (OVB)}
\end{flushleft}

\begin{wrapfigure}{R}{.4\textwidth}
\centering
\includegraphics[width=0.25\textwidth]{OVB}
\end{wrapfigure}
OVB occurs when both a a variable excluded from analysis (Z) both theoretically overlaps some of the variance in our dependent variable (Y) and another included variable (X). If Z only overlapped with Y excluding it would just lower our R$^2$ and we would lose some of our potential ability to explain our DV. If Z only overlapped our X variable then we need only acknowledge Z in theory. It is only when Z overlaps both that we run into problems, for both we lose explanation of our DV and we over-inflate the significance of X's influence on Y. This can (and always should) be corrected by building a model slowly, one variable at a time, alongside well thought theory to ensure accuracy and purpose in our problem solving.

\begin{flushleft}
\setlength{\parindent}{1cm} % 1cm indent reitterated after /wrapfigure

\noindent \textit{Parametric Linearity}\\
If we run a standard linear regression we should be sure that our theory informs the decision that this phenomenon is linear, but also check that our relationship between our variables is linear in nature too. Other modifications of the GLM framework can include non-linear distributions. Linearity in our variables can be aided via transformation, with a drawback being (potentially) in interpretation.

\hfill \\
\noindent \textit{Mathematical Concerns}\\
We should ensure that X and Y have enough variance to provide enough change for analysis. Also, we should ensure (as often as we can) that our number of observations (N) is larger than our parameters (K). While having too high of an N will be more likely to report significant results in nearly any scattering of points, too low of an N can either show sharply significant results or no results at all when the \textit{truth} can be the opposite. Typically having an N of 2,000 has been an accepted threshold.

\hfill \\
\noindent \textit{In Structure}\\
We also assume that our model is fully specified, and that our perameters ($\beta$) are ``fixed'' where our variables don't have a change-time or some external influence at a point in the data collection.

\hfill \\
\noindent \textit{Other Notes on Stochastic Robustness}

Note that much of the explainability and diagnostics of our model can be partially examined (at least as a first pass) through our R$^2$. Our R$^2$ is the amount of variance captured in Y from our model. More formally:
\begin{equation}
\mathlarger{ \text{R}^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1- \frac{\Sigma \epsilon^{2}_i}{\Sigma\left(Y_i - \bar{Y}\right)^2} }
\end{equation}
This shows our amount of variance explained by our model, over what's unexplained. This quotient is subtracted by one, we get a pretty good interpretation from .00 to 1 where we explain nothing to explaining everything. Other forms significance tests can penalize models for including many variables both linearly (AIC) or nonlinearly (BIC) these will be introduced later alongside logistic regression.

\hfill \\
\subsubsection{Missing Data}

\prac[inline]{Practice: How might we code and fill in missing data?}

Often is the case that we have missing data and still models are ran as usual. Yet, missing data very often occurs for a reason, be it a question offends a respondent, a respondent doesn't know how to answer, or a unit of observation intentionally does not respond because the response may not be acceptable to the researcher. That is, the missing data is very often not missing completely at random (MCAR). Rather, data can be missing with surrounding answered variables that we can use to predict the missing response (imputation). If we have no variables to attempt to predict the missing data (NI) then there is nothing we can do. With MCAR data we would go on as normal, or impute because it wouldn't really hurt anything.

Assuming we can use multiple imputation, here we want to run 5 to 10 models of the predicted missing response using any variables we have (here a ``kitchen sink'' approach to missing data is a good thing because we want to cover the highest amount of variance). With these 5-10 theoretical data sets that could exist where the respondent answered the otherwise missing question, we can regress each of these hypothetical worlds and then take the mean of all of them to have the most likely responses included in our final result.

Two R packages that make this very easy is both ``mice'' and ``Zelig.'' The functions to run multiple imputation from these packages can usually only take one or two lines of code and will print each individual regression and the mean regression. See the "mice" help file example example below:

<<imputation, prompt=TRUE, message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE, results=FALSE>>=
library(mice)
## Refering to the mice help file on imputation on medical data:
#  Note that all head() is just showing the top row results.

# do default multiple imputation on a numeric matrix
imp <- mice(nhanes)
head(imp)  # Checking, lots of useful information in here

@

<<imp2, prompt=TRUE, message=FALSE, warning=TRUE>>=
# Our data: note the NA missing values
head(nhanes$bmi)

# list the actual imputations for BMI, with predicted imputed values
head(imp$imp$bmi)

# first completed data matrix, can save this for analysis
head( complete(imp) )

## imputation on mixed data with a different method per column:
# mice(nhanes2, meth=c('sample','pmm','logreg','norm'))
## This will give you all information per column like before.

@


\clearpage

\subsubsection{Model Specificiation, Distributional Functions}

Some important continuous distributions:

\begin{longtable}{  m{2.5cm}  m{5.5cm} m{2cm}  m{2cm}  m{2cm}  m{2cm} }
\hline\noalign{\smallskip}
\textbf{Name} & \textbf{Density} & \textbf{Parameters} & \textbf{Mean} & \textbf{Variance} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Uniform &    $\frac{1}{b-a} \text{~ for~ } a < x < b$ & a,b & $\frac{a +b}{2}$ & $\frac{(b-a)^2}{12}$ \\
Triangular &  $\frac{1}{a} \left( 1- \frac{| x - b |}{a} \right) \text{, ~ for~} |x-b| < a $  &  a,b  &   b  & $\frac{a^2}{6}$  \\
Laplace \hspace{1cm} (Exponential) &   $ke^{-kx} \text{, ~ for~} x > 0 $ &  k  & $\frac{1}{k}$ &  $\frac{1}{k^2}$  \\
Normal (Gaussian) &   $\mathlarger{ \frac{1}{ \sqrt{2 \pi} \sigma} e^{- \frac{1}{2} \left(\frac{x-\mu}{\sigma} \right)^2} }$ &
$\mu, \sigma^2 $ & $\mu$ & $ \sigma^2 $ \\
Chi-Square  & $ \mathlarger{ \frac{x^{r/2-1} e^{-x/2}}{(v/2 - 1)! 2^{v/2}} }$  & $v$ & $v$ & $2v$ \\
Cauchy & $ \mathlarger{ \frac{1/\pi}{1+ (x-m)^2} }$ & m & (none) & (none)\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{longtable}

In the following sections each of these should be illuminated as we use them in regression and think about how these relate to the natural world.



%----------------------------------------------------------------------------------------

\clearpage
\subsection{Logit and Probit Modeling:}

Our dependent variable can take two values in logistic and probit regression; giving us a distribution such that:

\begin{gather*}
\begin{cases}
  1 = \text{for event occuring}    \\
  0 = \text{for event not occuring}
\end{cases}
\end{gather*}

\index{link function}
This can be used in a GLM framework where previously we had any continuous dependent variable available, we can have the dichotomous variable because of the necessary ``link function'' for our distribution. A link function is useful because it can be swapped out to fit any number of models we will see here in this document. This is absolutely important in GLM theory because it builds a connection between the mean of the data generating process to our linear predictors that otherwise would not pass our many assumptions if we were to just use standard OLS. Our two link functions for the logit and probit are as follows:

\begin{equation}
\mathlarger{ \text{logit:}~ \eta =  p\left( \frac{p}{1-p}\right) }
\end{equation}
\begin{equation}
\mathlarger{ \text{probit:}~ \eta = \Phi^{-1}(p) ~ \text{where}~  \Phi^{-1} ~ \text{ is the inverse normal distribution CDF} }
\end{equation}

Although probit and logit analyses are very similar in results, they differ in the distributions they allow. Probit regression works for a inverse normal distribution whereas logistic regression works for the logistic distribution (CDF's below). These are for the different theoretical DGP's, but the logit model is more likely to converge (data overlaps and varies enough) so we'll use that more often for accurate results, and often in social sciences we see the logit regression more often so given its canonical nature it is it more helpful at conveying important findings without confusion to the reader.

\begin{center}
\includegraphics[scale=.5]{VarLogit}
\end{center}

\clearpage

Below is an example of a model logit and probit regressions, reconsidering our beer data as the only real ranking of beer is ``Very Good'' or ``Fair''; after all a ``Good'' ranking may not make sense to be any different from ``Fair.'' Rather, these two rankings are combined to make a zero-one variable (``very good'' versus ``good or fair'').

<<logitprobit1, prompt=TRUE, eval=FALSE>>=
logmod1 <- glm(beer_data$verygood ~ beer_data$price,
               family = binomial(link=logit))
## summary(logmod1)  # Check your output

promod1 <- glm(beer_data$verygood ~ beer_data$price,
               family = binomial(link=probit))
## summary(posmod1)

# Plot:   Using library(texreg)
texreg(l= list(logmod1, promod1), stars = numeric(0),
       custom.model.names = c("Logit Model", "Probit Model"),
       custom.coef.names = c("Intercept", "Price"),
       caption.above = T, float.pos = "h!",
       custom.note = "Dependent variable: Very Good Beer (1)" )
@



\index{texreg package}
\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Logit Model & Probit Model \\
\hline
Intercept      & $-3.12$  & $-1.96$  \\
               & $(1.29)$ & $(0.75)$ \\
Price          & $0.75$   & $0.48$   \\
               & $(0.40)$ & $(0.23)$ \\
\hline
AIC            & 42.96    & 42.79    \\
BIC            & 46.07    & 45.90    \\
Log Likelihood & -19.48   & -19.39   \\
Deviance       & 38.96    & 38.79    \\
Num. obs.      & 35       & 35       \\
\hline
\multicolumn{3}{l}{\scriptsize{Dependent variable: Very Good Beer (1)}}
\end{tabular}
\end{center}
\end{table}

From the table we see things to be very similar between the two models, the error reports (AIC, BIC, Log Likelihood, Deviance) are all similar, our price coefficient is still in the same sign, the logit model barely avoids significance whereas the probit model barely makes significance standards. Both coefficients are in the same direction however. Our intercepts vary between each other, but both are negative and significant.

Reading dichotomous outputs like these can be difficult, for the logit model a coeficient is the \textbf{changes in logged odds from moving from zero to a one}. Therefore, our price of .75 is the logged odds change from moving from a zero (a ``Fair or Good'' beer) to a ``Very Good'' beer (1). But we often don't think in terms of logged odds, therefore it is always helpful to calculate the multiplicative effect of each variable.

We can try to understand more about what makes ``Very Good'' beer by building a model one block at a time until we think we have a complete model for the data we have - guided by our theory about very good beer in this case.

<<texexample, prompt=TRUE, eval=FALSE>>=
# Another texreg example: using library(texreg)
texreg(l= list(logmod2, promod2), stars = numeric(0),
       bold = T, ci.force = T,   # Note some of these changes here
       custom.model.names = c("Logit Model", "Probit Model"),
       custom.coef.names = c("Intercept", "Price", "Availability",
                             "Alcohol Content", "Caloric Content"),
       caption.above = T, float.pos = "h!",
       custom.note = "Dependent variable: Very Good Beer (1)" )
@


\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Logit Model & Probit Model \\
\hline
Intercept       & $\mathbf{-23.43}$ & $\mathbf{-14.03}$ \\
                & $[-43.03;\ -3.83]$    & $[-24.99;\ -3.07]$    \\
Price           & $1.11$                & $\mathbf{0.67}$   \\
                & $[-0.10;\ 2.33]$      & $[0.00;\ 1.33]$       \\
Availability    & $\mathbf{-3.53}$  & $\mathbf{-2.08}$  \\
                & $[-6.35;\ -0.71]$     & $[-3.63;\ -0.54]$     \\
Alcohol Content & $3.33$                & $1.94$                \\
                & $[-3.01;\ 9.67]$      & $[-1.59;\ 5.47]$      \\
Caloric Content & $0.04$                & $0.02$                \\
                & $[-0.10;\ 0.17]$      & $[-0.05;\ 0.10]$      \\
\hline
AIC             & 32.60                 & 32.20                 \\
BIC             & 40.37                 & 39.97                 \\
Log Likelihood  & -11.30                & -11.10                \\
Deviance        & 22.60                 & 22.20                 \\
Num. obs.       & 35                    & 35                    \\
\hline
\multicolumn{3}{l}{\scriptsize{Dependent variable: Very Good Beer (1)}}
\end{tabular}
\end{center}
\end{table}


The more specified table above includes the availability of the beer (is it often seen and promoted or more of an unknown beer to most bars), the alcohol, and caloric content; both of these are (at least chemically) to bring some form of pleasure and should be controlled. To switch up the data representation a little, confidence intervals are now shown and significance is indicated in bold.\footnote{ Texreg package customization, p.24 there is an example.} \index{texreg package}

Between the logit and probit models we still see a few differences, the intercept is again very different, likely because the difference in these models is the very distribution and link function they take which will shift the intercept; yet both are negative and significant. The price of the beer is insignificant for both models; yet, availability is significant for both models and in the negative direction (where 0 was a more frequent, popular beer). Alcohol and calorie levels are not significant for either model in predicting very good beers.\footnote{Interestingly caloric count is a low coefficient, likely because much of the variation is taken up by the alcohol content - which largely contributes to the calorie content in beer in the first place. An interaction between the two variables could be done, but this only inflates the alcohol content coefficient and all three variables become even further away from passing the significance threshold.} Finally, the model fitting reports are all very similar between the two models.

Going beyond sign and significance testing, let's first revisit our link function, if we take the inverse of the link function we can generate the ``mean function'' where instead of wrapping our link function around our linear predictor we can instead transform our DGP output itself. This can make more intuitive sense, and is again needed because then our values for $\eta$ will be from the zero to one interval.

\begin{equation}
\mathlarger{ \text{logit}^{-1} (\eta_i) = \frac{e^{\eta_i}}{1+e^{\eta_i}} \text{ to or from: } \eta_i = p\left( \frac{p}{1-p}\right) }
\end{equation}

Note from earlier that now in the logistic regression we are not dealing with a mere linear ``one unit in x causes an increase in y'' explanation. Rather, the logit regression is curved, and where we are at on the curve will determine how we read the regression coefficients. Generating predictive probabilities are always helpful, but require selecting a point of analysis per variable; this usually is set to the mean.

Consider our logit model from Table 4:

\begin{equation}
\mathlarger{ \text{logit}^{-1}(-23.43 + 1.11(\text{Price Value)} + \text{. . .} + 0.04(\text{Caloric Content))} }
\end{equation}

Here we could enter in mean numbers for each variable, and change the value we add per variable for interesting predicted probabilities in various scenarios that may be interesting to us. The number received is the probability of y happening given the parameters x given the data. This is both intuitive and interesting for readers.\footnote{Note this can also be done at the median of the data (intercept near zero), or at the middle category more generally. This is a case-by-case decision on whatever is most interesting given the question and data one is analyzing.}\index{``divide by four''}

The ``divide by four'' rule can be applied instead, were the beta coefficient is divided by four to yield the predictive probability \textit{at it's maximum in the middle of the curve}. At this point, directly in the middle (intercept = 0), the derivative of our function becomes equal to $\beta / 4$ allowing for an additional easy point estimate. Keep in mind this is best used for coefficients that are near the midpoint and not those on the tails because the logit distribution is much different at the tails than in the mid point. For example, in Table 4 our logit model showed the beer's availability had a large effect on the beer's rating. Taking it's availability $-3.53/4$ gives us a predictive probability of $-60.5$\%  That is, although this coefficient is away from the mean of the distribution, in example going from a widely available beer to a more sparse beer is likely to decrease the beer's rating (to the ``Good/Fair'' category) to a (approximate) 60.5\% probability.

Since it is usually better to represent these data in graphic form, consider a probability plot of our logistic curve for price now by the probability of receiving a very good beer:\footnote{The end section of this document, in quick overview, includes code for creating this.}


\begin{center}
<<probplotlogit, prompt=TRUE, out.width='5in'>>=

library(arm) # for inverse logit function
             # (It may actually be in the MASS package)

# Recall our logit model:
logmod1 <- glm(verygood ~ price, data = beer_data,
               family = binomial(link=logit))

# The inverse logit gets us our predicted probabilities:
#   All we're doing here is using some base-R plotting operations
#   with creating our predicted probabilities per our coefficents
curve(invlogit(logmod1$coef[1] + logmod1$coef[2]*x), 1, 5, ylim=c(-.01,.9),
      xlim=c(0,7.5), xaxt="n", xaxs="i", mgp=c(2,.5,0),
      ylab="Pr(Very Good Beer)", xlab="Price",
      main="Probability of Very Good Beer, by Price", lwd=4)
curve(invlogit(logmod1$coef[1] + logmod1$coef[2]*x), -2, 8, lwd=4.5, add = T)
axis(1, 1:7, mgp=c(2,.5,0))
mtext ("(two dollars)", 1, 1.5, at=2, adj=.5) # just adding text to x axis
mtext ("(six dollars)", 1, 1.5, at=6, adj=.5)
points(jitter(beer_data$price, 1.5), jitter(beer_data$verygood, 0.1),
       pch=20, cex=.9) # Jittered points so we can see them

@
\end{center}


We can run some diagnostics on this similar to linear regression in that we're using residuals which are calculated similarly but for logit regression we have to ``bin'' these. I've done so for the initial model (only including price) and the full model (including calories and alcoholic content) below.\footnote{For more on why binned residual plotting: see Andrew Gelman (et. al's) article:\\ http://www.stat.columbia.edu/~gelman/research/published/dogs.pdf}

Here, ideally, we would want to see ``random noise'' around the dotted zero mark more average residuals. Additionally, we want about 95\% of the data to be encapsulated within the light grey lines. Thus, interpreting the graphs, we see a lot of data that stretches outside of the confidence bounds within both graphics. If one were to collect more data in the beer dataset we may be able to analyze this further, but as it is there seems to be not enough samples of very good beer to create a complete image of the ranking process in this dichotomous way. \index{Binned Residuals}

\begin{center}
<<binned, prompt=TRUE, eval=TRUE, warning=FALSE, out.width='5in', out.height='4.5in'>>=
library(arm)

# Recall our logit models:
logmod1 <- glm(verygood ~ price, data = beer_data,
               family = binomial(link=logit))

logmod2 <- glm(verygood ~ price + avail + alcohol + calories,
               data = beer_data,
               family = binomial(link=logit))

## Binned Residual plotting ("arm" package)
par(mfrow=c(1,2))
x <- predict(logmod1, type = "response")
y <- resid(logmod1)
binnedplot(x, y, nclass=NULL,
           xlab="Expected Values", ylab="Average residual",
           main="Binned residual plot: Model 1",
           cex.pts=0.8, col.pts=1, col.int="gray")

# Plot 2:
x <- predict(logmod2, type = "response")
y <- resid(logmod2)
binnedplot(x, y, nclass=NULL,
           xlab="Expected Values", ylab="Average residual",
           main="Binned residual plot: Model 2",
           cex.pts=0.8, col.pts=1, col.int="gray")
par(mfrow=c(1,1)) # Reset graphing

@
\end{center}

%\begin{center}
%\includegraphics[scale=.7]{Rope}
%\end{center}

The below graphic compares the initial model with our full model in another great way, oftentimes readers prefer a visualization rather than tables. Our rope ladder plot shows just how little our two models differ, except in the wide-spread intercept, and few variables falling outside of the zero mark. Availability of the beer has a notable effect and is perhaps the main take away, along with price, of the logit models. High priced, widely available beer is very good in ranking (with some modeling concerns and error).
\index{Rope Ladder Ploting}

\begin{center}
\noindent\rule{8cm}{0.4pt}
<<rope, prompt=TRUE, warning=FALSE, message=FALSE, out.height='5in', out.width='5in' >>=
library(ggplot2)

# Rope Ladder Plot (ggplot2)
f1<-data.frame(Variable=rownames(summary(logmod1)$coef),
               Coefficient=summary(logmod1)$coef[,1],
               SE=summary(logmod1)$coef[,2],
               modelName= "Model 1")

f2<-data.frame(Variable=rownames(summary(logmod2)$coef),
               Coefficient=summary(logmod2)$coef[,1],
               SE=summary(logmod2)$coef[,2],
               modelName= "Model 2")

combinedframe <- data.frame(rbind(f1, f2))
interval1 <- qnorm((1-0.9)/2)
interval2 <- qnorm((1-0.95)/2)

rl1 <- ggplot(combinedframe)
rl1 <- rl1 + geom_hline(yintercept = 0, color = grey(1/2), lty = 5)
rl1 <- rl1 + geom_linerange(aes(x = Variable, ymin = Coefficient -
SE*interval1,
               ymax = Coefficient + SE*interval1),
               lwd = 1, position = position_dodge(width =1/2))
rl1 <- rl1 + geom_pointrange(aes(x = Variable, y = Coefficient,
ymin = Coefficient - SE*interval2,
               ymax = Coefficient + SE*interval2),
               lwd = 1/2, position = position_dodge(width = 1/2),
               color=("Black"),
               shape = 21, fill = "BLACK")
rl1 <- rl1 + coord_flip() + theme_bw()
rl1 <- rl1 + ggtitle("Comparing Two Models:")
print(rl1)

@
\end{center}







%----------







\clearpage
\subsection{Poisson Modeling:}
Poisson data uses a new link function which allows us to model count-level data, the probability mass function (pmf) now being:

\begin{equation}
\mathlarger{ \frac{\lambda^k e^{-\lambda}}{k!}     }
\end{equation}

Where the available numbers the model will take are all real numbers greater than zero $(0; 1; 2; 3; 4; 5; ...)$. This normal Poisson model assumes the variance is equal to it's mean, but often we see the variance to be greater (over-dispersed) or lower (under-dispersed) than the mean. This assumption is oftentimes broken in social sciences, for this reason a \textit{quasi-Poisson} or \textit{negative binomial} is able to let the variance change.
\begin{equation}
\text{Quasi-Poisson:} \hspace{.3cm} var(Y)=\theta \mu \hspace{.3cm}\text{Where theta is the overdispersion parameter}
\end{equation}
\begin{equation}
 \text{Negative Binomial:} \hspace{.3cm} var(Y)= \mu (1+ K\mu) \hspace{.3cm} \text{Where K is the shape parameter}
\end{equation}
How we handle the differences between the variance and mean relations in each of these three forms of count-data modeling may effect our regression coefficients fit differences between negative binomial (also known as gamma-Poisson), quasi-Poisson, and regular Poisson modeling. This is because fitting these models uses weighted least squares, and these weights are inversely proportional to the variance. Thus, negative binomial and quasi-Poisson will weight observations differently. Ultimately, the decision to choose the quasi-Poisson or another variation the negative binomial regression should depend on the theory rather than the data-driven method.\footnote{Ver Hoef, Jay M. and Boveng, Peter L., "QUASI-POISSON VS. NEGATIVE BINOMIAL REGRESSION: HOW SHOULD WE
MODEL OVERDISPERSED COUNT DATA?" (2007). Publications, Agencies and Staff of the U.S. Department of Commerce. Paper
142.}

Let's build a regular Poisson model, adding one variable at a time, to predict the likelihood of a successful military coup (R's faraway package). Let's assume that we theorize that the percent of a country voting, the years of oligarchy, number of elections, and political liberalization all make sense towards predicting the number (count) of successful military coups. Results are shown in Table 5 below, however here is how these models are ran in R:

<<Pois11, eval=TRUE, prompt=TRUE, message=FALSE>>=
library(faraway)
library(MASS)
data(africa)

## Examine Dataset ##
str(africa)  # There are some NAs in the data to consider

# Poisson
mod4 <- glm(africa$miltcoup ~ africa$pctvote + africa$oligarchy +
 africa$numelec + africa$pollib, family = poisson)

# Quasi Poisson
qmod4 <- glm(africa$miltcoup ~ africa$pctvote + africa$oligarchy +
 africa$numelec + africa$pollib, family = quasipoisson)

@



\hfill \\
\noindent \textbf{More on Dispersion Problems:}\\
In the Poisson distribution we make a rather strong assumption that our mean is equal to the variance due to the Poisson link function only having one parameter. We rarely ever fulfill this assumption in social sciences, but are able to report and correct for our dispersion problems pretty well. If our model is \textit{over-dispersed} this means that our mean is a certain distribution with a peak and the variance is larger than that actual mean distribution (not being equal as our assumption posits). The inverse is defined as under-dispersion although this is less common. A few ways to control for dispersion would be to check the F-statistic to report model-fit more accurately per variable as we're building our model. Also, we can check the typical robustness checks from the linear model including the QQ plot, Cook's Distance, and residual plotting. It may be the case that a logarithmic transformation (or similar transformation) of a variable will better-fit our model without actually changing and values given that they are all scaled together. Note how the regular Poisson assumption is a special case of the Quasi-Poisson regression (where $\theta$ equals one we arrive at the Poisson assumption $var(Y)=1\mu$. Often using Quasi-Poisson regression isn't a bad idea considering we rarely fulfill the Poisson dispersion assumption.\\

\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c c c }
\hline
 & Model 1 & Model 2 & Model 3 & Model 4 \\
\hline
Intercept                & $0.29$   & $-0.52$  & $-1.34$  & $-0.17$  \\
                         & $(0.28)$ & $(0.37)$ & $(0.61)$ & $(0.73)$ \\
Percent Voting           & $0.00$   & $0.00$   & $0.00$   & $0.00$   \\
                         & $(0.01)$ & $(0.01)$ & $(0.01)$ & $(0.01)$ \\
Years of Oligarchy       &          & $0.11$   & $0.13$   & $0.11$   \\
                         &          & $(0.02)$ & $(0.02)$ & $(0.02)$ \\
Number of Elections      &          &          & $0.10$   & $0.05$   \\
                         &          &          & $(0.05)$ & $(0.06)$ \\
Political Liberalization &          &          &          & $-0.40$  \\
                         &          &          &          & $(0.24)$ \\
\hline
AIC                      & 149.55   & 120.73   & 118.61   & 111.67   \\
BIC                      & 152.98   & 125.87   & 125.47   & 119.59   \\
Log Likelihood           & -72.77   & -57.36   & -55.31   & -50.83   \\
Deviance                 & 80.74    & 49.92    & 45.80    & 36.86    \\
Num. obs.                & 41       & 41       & 41       & 36       \\
\hline
\multicolumn{5}{l}{\scriptsize{Dependent variable: Successful Military Coups}}
\end{tabular}
\end{center}
\end{table}

We can include and check the chi-squared of our model since it is compatible to our Poisson distribution:\\
\begin{equation}
\mathlarger{ \hat{\phi} = \frac{\chi^2}{n-p} = \frac{\Sigma_i(y_i - \hat{\lambda_i})^2 /\hat{ \lambda_i}}{n-p} }
\end{equation}
More easily computed in R:\\

<<quickie, prompt=TRUE>>=
# ChiSquared of Model 4
dp <- sum(residuals(mod4, type="pearson")^2)/mod4$df.residual
dp
@

We can just run a quasi-Poisson family regression to get similar results initially. The follow is the two models comparatively:

\clearpage


\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Q-Model 4 & Model 4 \\
\hline
Intercept                & $-0.17$  & $-0.17$  \\
                         & $(0.78)$ & $(0.73)$ \\
Percent Voting           & $0.00$   & $0.00$   \\
                         & $(0.01)$ & $(0.01)$ \\
Years of Oligarchy       & $0.11$   & $0.11$   \\
                         & $(0.03)$ & $(0.02)$ \\
Number of Elections      & $0.05$   & $0.05$   \\
                         & $(0.06)$ & $(0.06)$ \\
Political Liberalization & $-0.40$  & $-0.40$  \\
                         & $(0.25)$ & $(0.24)$ \\
\hline
AIC                      &          & 111.67   \\
BIC                      &          & 119.59   \\
Log Likelihood           &          & -50.83   \\
Deviance                 & 36.86    & 36.86    \\
Num. obs.                & 36       & 36       \\
\hline
\multicolumn{3}{l}{\scriptsize{Dependent variable: Successful Military Coups}}
\end{tabular}
\end{center}
\end{table}

Notice that the coefficients don't change much at all, but reconsidering our measure of the variance has changed some significance levels slightly. Note we cannot see our AIC, BIC, or Log Likelihood for the quasi-Poisson; this is because of how the variance is manipulated, a Quasi-AIC measure would be needed to report that finding. Also, we could include a negative binomial model next to the others:

<<Pois1, eval=FALSE, prompt=TRUE>>=
# Poisson
mod4 <- glm(africa$miltcoup ~ africa$pctvote + africa$oligarchy +
 africa$numelec + africa$pollib, family = poisson)

# Quasi Poisson
qmod4 <- glm(africa$miltcoup ~ africa$pctvote + africa$oligarchy +
 africa$numelec + africa$pollib, family = quasipoisson)

# Negative Binomial
nbmod4 <- glm.nb(africa$miltcoup ~ africa$pctvote + africa$oligarchy +
 africa$numelec + africa$pollib)
@

\clearpage

\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c c }
\hline
 & Model 4 & Q-Model 4 & Negative-Binomial Model 4 \\
\hline
Intercept                & $-0.17$  & $-0.17$  & $-0.17$  \\
                         & $(0.73)$ & $(0.78)$ & $(0.73)$ \\
Percent Voting           & $0.00$   & $0.00$   & $0.00$   \\
                         & $(0.01)$ & $(0.01)$ & $(0.01)$ \\
Years of Oligarchy       & $0.11$   & $0.11$   & $0.11$   \\
                         & $(0.02)$ & $(0.03)$ & $(0.02)$ \\
Number of Elections      & $0.05$   & $0.05$   & $0.05$   \\
                         & $(0.06)$ & $(0.06)$ & $(0.06)$ \\
Political Liberalization & $-0.40$  & $-0.40$  & $-0.40$  \\
                         & $(0.24)$ & $(0.25)$ & $(0.24)$ \\
\hline
AIC                      & 111.67   &          & 113.67   \\
BIC                      & 119.59   &          & 123.17   \\
Log Likelihood           & -50.83   &          & -50.83   \\
Deviance                 & 36.86    & 36.86    & 36.85    \\
Num. obs.                & 36       & 36       & 36       \\
\hline
\multicolumn{4}{l}{\scriptsize{Dependent variable: Successful Military Coups}}
\end{tabular}
\end{center}
\end{table}


Ultimately, for publication I would include all of these in an appendix or supplemental appendix. Our results themselves are largely insignificant and our coefficients, using $\mathlarger{ e^{\beta} }$ we can find the multiplicative change of each variable's coefficient. The most significant finding (consistently) is the years of oligarchy a country experiences predicts the multiplicative likelihood of a military coup. For interpretation, our $\beta = .11$ so $e^{.11} = 1.116278$ or an 12\% increase in the likelihood of a military coup as years of oligarchy increases. In Table 8 I do what is most intuitive for readers, report a model (in this case the negative binomial) and it's associated percentage likelihood estimate.
\clearpage


\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Negative-Binomial Model & $1- e^\beta$  \\
\hline
Intercept                 & $-0.17$    & \\
                          & $(0.73)$   & \\
Percent Voting            & $0.00$     & $0\%$ \\
                          & $(0.01)$   &\\
Years of Oligarchy        & $0.11$     & $12\%$ \\
                          & $(0.02)$   &\\
Number of Elections       & $0.05$     & $5\% $ \\
                          & $(0.06)$   &\\
Political Liberalization  & $-0.40$    & $-33\%$\\
                          & $(0.24)$   &\\
\hline
AIC                       & 113.67     &\\
BIC                       & 123.17     &\\
Log Likelihood            & -50.83     &\\
Deviance                  & 36.85      &\\
Num. obs.                 & 36         & 36\\
\hline
\multicolumn{3}{l}{\scriptsize{Dependent variable: Successful Military Coups}}
\end{tabular}
\end{center}
\end{table}


%--------------------------------------------------------------------------------------------
\addcontentsline{toc}{subsubsection}{Likelihood Ratio Test, Deviance, AIC, and BIC}
\noindent \textbf{Likelihood Ratio Test, Deviance, AIC, and BIC:}\\
In the likelihood ratio test we have two models where one is a subset (or nested) within another (following is using hypothetical data). Here we have the likelihoods of both of the restricted model ($\hat{L_R}$) divided by the unrestricted model ($\hat{L_U}$) thus taking their ratio. Using hypothetical data our restricted model logged-likelihood is -469.174 and our unrestricted model is a logged-likelihood of -406.0718. So assume here we have a restricted model with a higher logged-likelihood than the unrestricted model which isn't ideal of a likelihood ratio test. Further, our logged-likelihoods are negative which is also not ideal in creating an interpret-able likelihood ratio test. Exponentiating each of these:\\
\begin{equation}
\mathlarger{ \lambda = \frac{ \hat{L_R}}{ \hat{L_U}} = \frac{ e^{-469.174}}{e^{-406.0718}} = 3.9360695\text{e}-28 }
\end{equation}

Given that this doesn't really work, and our results would be helpful but we would have to just interpret arbitrary an arbitrary ``cut-off" point between zero and one to determine if we can reject the null or not, we can actually just use a \textbf{logged-likelihood ratio test} which is similar but not exactly a likelihood ratio (although this is more common):\\
\begin{equation}
\mathlarger{  \lambda = -2 \ln \left( \frac{ \hat{L_R}}{ \hat{L_U}}  \right) = -2 \left[(-469.174) - (-406.0718) \right] = 126.2044  }
\end{equation}

\clearpage

We could run this number in R: \\

<<dchi, prompt=TRUE >>=
dchisq(126.2044, df=1)
@


Similarly we can calculate the deviance, or the difference between a perfect fitting model and our models. Given that a perfect fitting model (for general linear models at least) are by definition perfect, the math becomes easier because perfect models can be understood as zero: \\
\begin{equation}
D = 2(0- \ln L(\hat{\theta} \vert y)
\end{equation}
\begin{equation}
 D = -2( \ln L(\hat{\theta} \vert y)
\end{equation}
Although the calculation doesn't involve pitting the models against each other we can compare model's distance away from the ideal models. We can also look at the AIC and BIC, defined similarly:\\
\begin{equation}
AIC = -2 \ln L(\hat{\theta} \vert y) + 2p = D(\hat{\theta} ) +2p
\end{equation}
\begin{equation}
BIC = -2 \ln L(\hat{\theta} \vert x) +plog(n) = D(\hat{\theta}) + p \ln (n)
\end{equation}


These are important to apply because if we arbitrarily add more and more variables into a model, our deviance will continue to decrease although this may not be honest to our actual understand of a ``better'' model (kitchen-sink models). AIC adds $2p$ where p is the number of parameters; thus, this adds a linear check against our deviance. BIC adds $p\cdot log(n)$ or a non-linear check on our deviance. Together than can help us check our model fit (and is often automatically calculated for you in tables in R) \\

Overall, the AIC, BIC, Log Likelihood, and Deviance are all great indicators for model fit from the ideal model and checking for many parameters (both linearly and non-linearly).

















\clearpage
\subsection{Ordered Logistic Modeling:}

Ordered models take data that has a scale in responses, where a ``1'' response is less than or greater than a ``2'' response which is in similar direction to a ``3'' response. These ordered responses, in order to work in a GLM framework, must be formulated to be measurable at each disjointed segment between each ordered response. It may be helpful to think of this as a series of logitstic regressions between the segments, in the end bonded together. Therefore, we get the link function for the ordered logistic:\\ \index{link function}
\begin{equation}
\mathlarger{ \frac{1}{1+ e^{-(\theta_i- w\cdot x)}   }  }
\end{equation}
\noindent and (optionally) the ordered probit could be used, or a sometimes attractive third option which is the exponential function to create a proportional hazards model, although survival models will be covered at the end of this document these two models are relatives of one another:\\
\begin{equation}
exp(-exp(\theta_i - w \cdot x))
\end{equation}
Intuitively, with a series of logistic regressions we could subtract these categories to find the effects of each category; or add these categories up to equal 1. If these categories do not add up to one the remaining number is the coefficient for a left out category. Subtracting these categories requires subtracting the inverse logit of each: $logit^{-1}(\theta_{k+1}) - logit^{-1} (\theta_k)$. AIC and BIC, note, will be including all disjointed segments. Also, by doing a regression we are not finding the areas between these ``cut points" but rather the label (or response) at each cut point itself. We can take the derivative of the line estimate between cut points to find the nuanced area in between cut points, but subtracting the inverse logits (above) is essentially the same result.

Revising the data on beer, we should realize that our ranking variable is directly suited for ordered logistic regression (not OLS). Let's see if our findings differ in any way from the OLS or logit models we ran. Consider an ordinal logit of the beer ranking:



\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Model 1 & Model 2 \\
\hline
Price           & $0.74$   & $0.63$   \\
                & $(0.36)$ & $(0.36)$ \\
Alcohol Content &          & $1.75$   \\
                &          & $(0.82)$ \\
\hline
AIC             & 76.76    & 72.04    \\
BIC             & 81.42    & 78.27    \\
Log Likelihood  & -35.38   & -32.02   \\
Deviance        & 70.76    & 64.04    \\
Num. obs.       & 35       & 35       \\
\hline
\multicolumn{3}{l}{\scriptsize{Dependent variable: Quality of Beer Ranking}}
\end{tabular}
\end{center}
\end{table}



Interpretation of the results are similar to logit regression in that they are the logged odds change from one cut point to another cut point. That is, from a ``2'' quality to a ``3'' we see a 1.75 logged percentage increase per unit increase in alcohol content. This effect is statistically significant.

\addcontentsline{toc}{subsubsection}{Bootstrapping}
\hfill \\
\noindent \textbf{Bootstrapping:}

One issue to consider is the low observation number (N) but high demand of the model (varying categorical demands of our dependent variable). One robustness check of this is to \textit{bootstrap} the model to use simulation to build up the number of observations to fulfill our assumption of normal distribution which may be a hard assumption without doing this:
\begin{center}
\includegraphics[scale=.65]{best}
\end{center}
Above, we see our beta estimate is not a very good normal distribution even though our model assumptions assume that we should treat it like one. In order to make the assumption of normal distribution we can bootstrap the model by replicating it's draws. Figure 2 maps a normal distribution and marks the same model's beta coefficients information to show an ideal model:

\clearpage

\begin{center}
\includegraphics[scale=.6]{Nbest}
\end{center}

Using these new-found thresholds, including a red mean line ideally where it should between the blue lower and upper bounds, we can re-run our model based upon the new information - creating new high and low confidence intervals, we find that significance still remains in Table 10: \\

\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c }
\hline
 & Model 1 \\
\hline
Price           & $0.63$           \\
                & $[-0.08;\ 1.95]$ \\
Alcohol Content & $1.75^{*}$       \\
                & $[0.62;\ 4.96]$  \\
\hline
AIC             & 72.04            \\
BIC             & 78.27            \\
Log Likelihood  & -32.02           \\
Deviance        & 64.04            \\
Num. obs.       & 35               \\
\hline
\multicolumn{2}{l}{\scriptsize{Dependent variable: Quality of Beer Ranking}}
\end{tabular}
\end{center}
\end{table}


This will typically yield more conservative estimations and evaluations of our standard errors, commonly in cases with low N. Code for this process:

<<orderd1, prompt=TRUE, warning=FALSE, message=FALSE, eval=TRUE>>=
set.seed(10)
# This seed is used, it assures replication and convergence for this model

# For Bootstrapping:
bs.sample <- function(dat)
{
  idx <- sample(1:nrow(dat), nrow(dat), replace = TRUE)
  dat[idx,]
}

bs.routine <- function(dat)
{
  sample.dta <- bs.sample(dat)
  coef(MASS::polr(as.factor(quality) ~ price + alcohol,
                  data = sample.dta))
}
bs_est <- replicate(1000, bs.routine(dat = beer_data))
# replicate is actually doing the bootstrap^

bs_mean <- rowMeans(bs_est)
bs_ci <- t(apply(bs_est, 1, quantile, probs = c(0.025, 0.975)))
bs_results <- cbind(bs_mean, bs_ci)
colnames(bs_results) <- c("Est", "Low", "High")

# Model build
mod11 <- MASS::polr(as.factor(quality) ~ price + alcohol, data=beer_data)
# Quick Results
texreg::screenreg(mod1,
                  override.ci.low = c(bs_results[, 2]),
                  override.ci.up = c(bs_results[, 3]))

# Table, to show overriding the confidence intervals with the bootstrap
# library(texreg) # For this:
# texreg(l= list(mod11), stars = numeric(0),
#        custom.coef.names = c("Price", "Alcohol Content"),
#        override.ci.low = c(bs_results[, 2]),
#        override.ci.up = c(bs_results[, 3]),
#        caption.above = T, float.pos = "h!",
#        custom.note = "Dependent variable: Quality of Beer Ranking" )

@




\noindent \textbf{Model Interpretation} \\
Assume R gives us the following raw output involving giving aid to Katrina relief:
\begin{verbatim}
Model:
               Coefficient    Std. Error    0.95 CI Lower     0.95 CI Upper
alt              1.2024         0.6335         -0.0393            2.4441
age01            3.4481*        1.1723          1.1504            5.7458
female           0.9407*        0.4333          0.0915            1.7899
inc8cat         -0.0188         0.7181         -1.4262            1.3886
advdeg          -0.9940*        0.4894         -1.9531           -0.0348
nonwhite        -0.5211         0.4990         -1.4991            0.4569
pid7cat          0.7343         0.9204         -1.0697            2.5384
0   | 0.5        0.9497         0.7954         -0.6092            2.5086
0.5 | 1          1.5801         0.8065         -0.0007            3.1608
Log Likelihood:      -89.63342 (df=9)
\end{verbatim}

Three variables are within two standard errors, age, if the respondent is female, and their average degree held. Interpreting ordered coefficients, since our dependent variable is ordered and many of our independent variables are ordered we will often care about the inverse logged difference between variables to find the rate of change between one ordered ``cut-point'' to the next. The difference between these two cut points recall:\\
\begin{equation}
logit^{-1} (\theta_{k+1}) - logit^{-1}(\theta_k)
\end{equation}
Where ``$k$" is the comparable category across the variable of interest. For example, we can compare our $\beta$ from our zero to $.5$ cut point to our $.5$ to 1 cut point in our dependent variable: donating to Katrina relief.
\begin{equation}
logit^{-1} (.9497) - logit^{-1}(1.5801) \approx (8.9) - (38) \approx   -29.1
\end{equation}
The rate of change between our .5 cut point (considering donating) and our cut point of 1 (has already donated) is -29.1. This predictive estimation is \textbf{all in reference to the omitted category}: zero or those who do not plan to donate given that all other variables are set at one. We find that having a low altruism score, being a minority race, and having a low average degree but highly liberal are all characteristics of someone that is far less likely to donate - or for us to observe a shift from a .5 to a 1 in the donation categories.

However, predicting a category like altruism (which ranges from 1-10) can be far different. Given the range of categories the variable could just be treated just as a variable would in a standard linear model, but the interpretation again is the \textbf{relative change from the removed category 0} (not considering a donation). In this instance we see a 1.2 percentage increase in the logged likelihood of donating for every unit increase in altruism. \\

\clearpage
\noindent \textbf{Latent Spaces}\\
This question has come up in the previous model's interpretation. What we obtain from a categorical variable is various ``cut points'' or snapshots of some longer distribution drawn out between these cuts in our distribution. These thresholds only tell us the associative values at each of these points, it is on the researcher to either integrate or use the difference in inverse logs to find the rate of change between points - or the latent spaces between the thresholds. \\
The more categories we add in initial data collection the easier it becomes to measure these latent spaces, for example age (a continuous variable) could be categorized as Teen, Young Adult, Adult, Elderly. Or, we could collect more data and have it by year - or by day! Measuring the difference between groups, say form Adult to Elderly, would take the probability at the stage of being an adult and everything greater than it ($k+1$) minus being Elderly and everything less than it ($k$) and then we find the effect from one threshold to the next. As this crude sketch shows, this is important because our distribution \textit{between} cut points could be interesting to understand.\\

\begin{center}
\includegraphics[scale=.65]{yeah}
\end{center}








%------






\clearpage
\subsection{Multinomial Nominal Modeling:}

In multinomial modeling we have categories that have no particular order among them (religious identification for example, or other nominal variables). This is another extension of the binomial regression, except in 0-1 we have 0 all the way up to the $j$th group. The probability of all of these categories have to sum to 1. To use the GLM framework we use the following link function:\\
\begin{equation}
\eta_{ij} = x^{T}_i \beta_j = log \frac{p_{ij}}{p_{i1}} \hspace{.5cm} \forall  \hspace{.5cm}  j = 2, ... , J
\end{equation}
Since these categories have to equal to 1, we can manipulate and set the baseline category (usually whatever is coded 1 in R) to come up with a link that looks very similar to the logistic link, only with a summation operator for the categories starting beyond the omitted category, at $j=2$ to the final category $J$:\\
\begin{equation}
 p_{ij} = \frac{exp(\eta_{ij})}{1+ \Sigma^{J}_{j=2} exp (\eta_{ij})   }
\end{equation}
The only difference between this and the logistic regression is the summation operator for multiple categories rather than just the two in logistic regression. To run multinomial data in \texttt{R} we use the \texttt{nnet} package, which includes a lot on neural networks but also has a great multinomial function:\\


<<multinom1, prompt=TRUE, eval=FALSE>>=
library(nnet)
object <- multinom(formula)
@


The following table is what a multinomial output looks like, created using the \texttt{R faraway} package, utilizing the NES 1996 data. Assume, for some reason, all we needed to add in our model was age and the amount of TV someone watched to predict their partisan leanings. \\



\begin{table}[h!]
\caption{Statistical models}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Independent & Republican \\
\hline
Intercept       & $\mathbf{-2.71}^{*}$ & $-0.36$          \\
                & $[-3.73;\ -1.69]$    & $[-0.76;\ 0.04]$ \\
Age             & $0.01$               & $0.01$           \\
                & $[-0.02;\ 0.03]$     & $[-0.00;\ 0.02]$ \\
TV News         & $-0.06$              & $-0.05$          \\
                & $[-0.19;\ 0.08]$     & $[-0.10;\ 0.01]$ \\
\hline
AIC             & 1571.79              & 1571.79          \\
BIC             & 1600.89              & 1600.89          \\
Log\ Likelihood & -779.90              & -779.90          \\
Deviance        & 1559.79              & 1559.79          \\
Num.\ obs.      & 944                  & 944              \\
\hline
\multicolumn{3}{l}{\scriptsize{$^*$ 0 outside the confidence interval}}
\end{tabular}
\end{center}
\end{table}

The raw coefficient we get is the logged odds from moving from the deleted category (Democrat here) to either Independent or Republican. Logged odds are pretty unituitive however, so making predictive probabilities from multinomial coefficients (and indeed much of regression output) is helpful to readers:

\begin{center}
\includegraphics[scale=.7]{IncPlot}
\end{center}

The above plot is modeling income along the x axis and the probability of being Democrat, Republican, or Independent (purple) for the NES 1996 data. This is using the ``predict'' command in R, which generates predictive probabilities out of our coefficients.

<<multinom2pr, prompt=TRUE, eval=FALSE>>=
predict(mmodi,data.frame(nincome=il),type="probs") # include type="probs"
predict(mmodi,data.frame(nincome=il))

# Create Object with this probability matrix
prmat <- predict(mmodi,data.frame(nincome=sort(unique(nincome))),
         type="probs")

# Plotting:
plot(unique(nincome), prmat[,1], type="l", col="blue", lwd=3,
       xlab="Income", ylab="Probability") # Labels
lines(unique(nincome), prmat[,3], type="l", col="red", lwd=3)
lines(unique(nincome), prmat[,2], type="l", col="purple", lwd=3)

summary(mmodi)
@








%-----------







\clearpage
\subsection{Survival Modeling:}

Survival models (event history models) take data that occurs over a duration of time and maps the likelihood of surviving until the next ``step''. This could be, for example, the effect of a treatment, the lifeline of a patent in therapy, the tenure of a officeholder, or the length that a bill or law stays active over time. Consider the table:

<<survmod1, prompt=TRUE, eval=FALSE>>=
library(survminer)
library(survival)

# Expoential:
expmod <- survreg(Surv(te, censor) ~ wc + dem + pvote + south,
                  data = dat, dist = "exponential")

# Weibull:
weibullmod <- survreg(Surv(te, censor) ~ wc + dem + pvote + south,
                  data = dat, dist = "weibull")
@



\begin{table}[h!]
\caption{Survival Models}
\begin{center}
\begin{tabular}{l c c c }
\hline
 & Exponential & Weibull & exp(Cox) \\
\hline
Intercept          & $3.60$    & $3.59$    &           \\
                   & $(0.10)$  & $(0.10)$  &           \\
Investiture        & $-0.36$   & $-0.34$   &           \\
                   & $(0.13)$  & $(0.12)$  &           \\
Polarization       & $-0.039$  & $-0.04$   & $1.04$    \\
                   & $(0.01)$  & $(0.005)$ & $(0.01)$  \\
Crisis Duration    & $0.01$    & $0.01$    & $0.99$    \\
                   & $(0.002)$ & $(0.002)$ & $(0.002)$ \\
Log(Scale)         &           & $-0.12$   &           \\
                   &           & $(0.05)$  &           \\

\hline
Scale              & Fixed 1   & 0.884     &         \\
Log\ Likelihood    & -1056.7   & -1053.8   & 56.6    \\
$\chi^2$           & 88.08     & 93.6      &         \\
df                 &  3        &   3       & 2       \\
Num.\ obs.         & 314       & 314       & 314     \\
\hline
\multicolumn{4}{l}{\scriptsize{Dependent variable: Duration}}
\end{tabular}
\end{center}
\end{table}

The above table is the coefficients and standard errors for two models (and an expoentiated Cox model), the exponential and Weibull of a survival modeling of duration given legislative investiture, polarization measures, and crisis duration (measured in days). Our variable of interest here is the presence of legislative investiture and Gary King's claim that they prevent against failure in international national cabinets (their duration).\footnote{King, Gary, James E. Alt, Nancy Elizabeth Burns and Michael Laver (1990). ``A United Model of Cabinet Dissolution in Parliamentary Democracies," American Journal of Political Science, vol. 34, no. 3, pp. 846-870.}

In interpretation, The exponential model assumes the scale parameter to be uniform for our hazard model (uniform at 1). This, in a basic sign-and-significance interpretation yields negative relationships for investiture policy and polarization which are both significant but weakly influential. The Weibull model gives us the actual scale parameter (.884) and similar results although slightly weaker when accounting for distributional changes additionally the interpretation changes via in absolute value terms from the exponential to the Weibull model. The Cox model assumes that the two strata within the survival curves will be proportional to each other, which could be an easy assumption to fulfill in some theory. Finally, the exponentiation of the Cox model can be read similarly to the Poisson model and is just a measure \textit{of the effect of the hazard rate}, which again is the risk of failure given that the participant has survived up to a specific time. The Cox model output here does not include our strata (investiture).\footnote{\texttt{coxph(Surv(duration, ciep12) $\sim$ strata(invest) + polar + crisis, data = dat,\\
 method = "efron")}}


\begin{center}
\textbf{Figure 1: Exponential Survival Model of Legislative Investiture}
\includegraphics[scale=.65]{Surv}
\end{center}


\clearpage
\begin{center}
\textbf{Figure 2: Cox Survival Model of Legislative Investiture}
\includegraphics[scale=.65]{Cox}
\end{center}


<<survmodel, prompt=TRUE, eval=FALSE>>=
#Survival GGplot Example (not used to replicate above, code as template)

    ggsurvplot(fit, data = dataset)
ggsurvplot(fit, data = dataset,
                surv.median.line = "hv",  # Add medians survival
                        # Change legends: title & labels
                legend.title = "Title",
                legend.labs = c("One Category", "Another"),
                        # Add p-value and confidence intervals
                pval = TRUE,
                conf.int = TRUE,
                        # Add risk table
                risk.table = TRUE,
                tables.height = 0.2,
                tables.theme = theme_cleantable(),
                #Load "GrandBudapest" color scheme
                palette = c(wes_palette("GrandBudapest", 2,
                type = "discrete")[1],
                wes_palette("GrandBudapest", 2,
                type = "discrete")[2]),
                ggtheme = theme_bw()  #Change ggplot2 theme
              )
@


\newpage

Survival modeling is a variation of the generalized linear model which accounts for predicted probabilities as a phenomenon or actual actor dies in the course of study. Each depression in the figures is the instance of death in the dependent variable. Exponential modeling holds the hazard as uniform (1) giving us an interpretation of the predicted chance of dying. Weibell, in contrary, is the predicted probability of living and does not make an assumption on the scale parameter and reports this hazard parameter to the researcher (resulting in a loss of one degree of freedom).\footnote{Recall that this is very similar to the scale/variance problem we occurred in Poisson regression. Similarities are very similar here as well, and the math is similar in this respect too.} Confidence intervals for both Figure 1 and 2 are set at 95\% and the number at risk are the remaining participants in the study at each time location. The dotted lines report the median of the data for each strata (Investiture). Finally, given the Kaplan-Meier modeling here each depression is robust in that it controls for a multiple-drop off scenario to where $1/n$ can be calculated. For example, if three people were to die off at the same time point a $1/10, 1/9, 1/8$ would be calculated instead of all three at the same functional moment. The number remaining at risk is shown descriptively below each figure. \\

Interpretation of the actual coefficients is very similar to Poisson modeling given that our general linear model is following similar distribution as the Poisson (non-negative). Yet, the Poisson uses count-integer level data whereas survival modeling can use continuous data. Because of the similarities, the beta coefficient reported is similar, it is the multiplicative effect holding all else constant of a one unit increase in $X$ onto $Y$. \\

I have been able to plot both of these together in base-R; but I don't like how it looks compared to \texttt{ggplot}. \\


\begin{center}
\textbf{Figure 3: Survival Models of Legislative Investiture in Base R}
\includegraphics[scale=.6]{SurvR}
\end{center}

\clearpage
\noindent \textbf{Further Notes: Comments}\\
\noindent \textit{Censoring Problem in Survival Models:} \\
Perhaps as an additional note, a unique problem to survival modelings is the issue of censoring. Censoring is where participants (or the observed phenomenon)lives beyond the end point of the study. This is an example of rightward-censoring, but leftward-censoring would just be the inverse (beginning a study beyond the start of the phenomenon). This is a unique and difficult threat to survival models, thus we have had to use much of the tools listed in this section to account for these kinds of data. \footnote{Princeton article here includes mathematically how this and much of survival analysis works:\\
 http://data.princeton.edu/wws509/notes/c7.pdf}

\hfill \\

\noindent \textit{iid:} \\
The GLM nature of the survival model does not give it a free pass on the independent and individually distributed (iid) assumption. In fact, much of the time in dealing with survival-type theories (bill-survival, authoritarian regimes, cancer patients) we deal with responses that are not independent of one-another. A doctor seeing cancer patients, for example, may see twenty patients and as they become fully treated or pass away he may have more time to distribute to other patients. These patients may even meet and have group-therapy among themselves (just as bills may co-vary and authoritarians interlock in strategy)! Further methodology (networks for instance) can help circumvent these concerns.

















\clearpage
\subsection{Conclusion, Quick Reference}

\textbf{The GLM}\\

The general linear model framework consists of a stochastic and systematic component, in order to use other distributions in the exponential family we need a \textit{link function} and to change our distriubtion. These build connections (links) between the mean of the DGP to our linear predictors.\\

\hfill \\
\noindent \textbf{Linear Model}\\
It's origins in geometry, the standard linear model is initially pretty flexible to most social science data. In geometry, $y=mx+b$ can be rewritten for data: $y=\alpha + \beta X$ or $y= \beta_0 + \beta X$. where $\alpha$ or $\beta_0$ is the y-intercept, $\beta_i$ is the $i$th coefficient for each variable onto our dependent variable $y$ and $X$ is our observed independent variable data. \\
In R this is executed simply:\\
\begin{verbatim}
object <- lm(formula)
#formula: (DV ~ IV + IV + IV)
\end{verbatim}

Coefficients received in a standard linear model table are interpreted as a one unit increase in X as an effect on our DV (pg. 4 for more, and probability plotting). \\

\hfill \\
\noindent \textbf{Logit Regression:}

Logit (and the alternative probit) regression(s) examine dependent variables that are coded dichotomous (0;1). This is accomplished by the link function $p\left(\frac{p}{1-p}\right)$.

 If coded correctly, logit regression can give you the logged odds from moving from a zero (event not happening) to one (event happening). Yet, we often don't think in terms of logged odds, so we can calculate predictive probabilities by dividing the coefficient by four - this will yield a maximum predictive probability when the coefficient rests in the middle of the logistic curve. \\

 More helpful for readers, perhaps, is to generate a probability plot, including data points along the x axis and the probability(y) along the logistic curve on the y axis. \\

\clearpage
\noindent \textbf{Poisson Regression:}\\
Poisson, Quasi-Poisson, and Negative Binomial regression all take count level data. Poisson assumes the variance to be held equal to the mean (a ``1'' in Quasi-Poisson); whereas Quasi-Poisson and Negative Binomial allow the variance to differentiate between models. In R:\\

<<Pois Running, eval=FALSE>>=

# Poisson
object <- glm(formula, family = poisson)

# Quasi-Poisson
object <- glm(formula, family = quasipoisson)

# Negative Binomial/ Gamma-Poisson
object <- glm.nb(formula)

@

For interpretation, it is best to exponentiate the coefficients ($e^\beta$) to receive the multiplicative change exerted from each variable (keeping in mind significance). Tables can be created that show readers these effects (pg. 18-19).


\hfill \\
\noindent \textbf{Ordinal Regression:}\\
Ordinal data requires the interpretation of cut points ($k$) within our regression and is interpreted similarly to logistic regression. Ordinal data between cut points can be thought of as a series of logistic regressions between cut points.


<<ord, eval=FALSE, prompt=TRUE>>=
# Ordinal Regression
object <- polr(formula)
@


It is important to make note of the ordinal categories, one category is omitted as a reference category within regression and the information received is in refference to the omitted category. Inverse logit subtraction can yield predictive probabilities: $logit^{-1}(\theta_{k+1}) - logit^{-1}(\theta_k) $ these latent space estimates are important to note as, when controlling for all else, the relative change between ordinal responses (feeling ``okay'' to feeling ``great'' for example).

\hfill \\




\noindent \textbf{Multinomial Nominal Modeling:}

<<nnet, eval=FALSE, prompt=TRUE>>=
library(nnet)
object <- multinom(formula)
@


Multinomial modeling includes data that is non-ordered, for example religious identity. Mathematically the link function (considering all probabilities for each category need add up to one) is very similar to the logistic link.

With the raw coefficients we get from multinomial data we receive the logged odds (like logistic regression) from moving from the omitted category (usually the first category in R) to the following categories. Again, we don't typically think in logged odds (another similarity to logistic regression) so making predictive probability plots are helpful (again, similar to logistic regression). \\


\hfill \\
\noindent \textbf{Survival Modeling: }

Survival models (as the name suggests) measures and indeed explicitly addresses the length of time a unit dies or survives across a length of time. The hazard parameter is held constant in the exponential model (at 1) whereas in the Weibull model it is allowed to vary (losing one degree of freedom, but this choice would depend on theory). To model survival data:

<<survmod3, prompt=TRUE, eval=FALSE>>=
library(survminer)
library(survival)

expmod <- survreg(Surv(te, censor) ~ wc + dem + pvote + south,
                  data = dat, dist = "exponential")

weibullmod <- survreg(Surv(te, censor) ~ wc + dem + pvote + south,
                  data = dat, dist = "weibull")
@



In interpretation of these models' output, we should consider it similar to the Poisson modeling: the multiplicative effect, holding all else constant, of a one unit increase in X onto Y. Although predictive probability plots are (again) superior to present these results.

Mapping graphs as shown in full in Section 7, are a intuitive and attractive way to communicate results to the reader. In these we separate two interesting strata and map the survival data accordingly, controlling for what we deem important with our theory. Kaplan-Meier \index{Kaplan-Meier} checks are also important because multiple ``deaths'' could occur at once and this will scale each of these instead of all at once - which is better.









\clearpage
\section{Machine Learning}

\textit{Types of Machine Learning}
\begin{equation}\nonumber
\begin{cases}
\text{Supervised learning} \begin{cases} \text{Classification} \\ \text{Regression} \end{cases}\\
\text{Unsupervised learning} \begin{cases} \text{Discovering clusters} \\ \text{Discovering latent factors} \\ \text{Discovering graph structure} \\ \text{Matrix completion} \end{cases}\\
\end{cases}
\end{equation}


\subsection{Simulation, Replication, Bootstrapping}

\subsection{Cross Validation}

\subsection{Penalized Regression}

\subsection{K Nearest Neighbors, Nearest Means}

\subsection{Support Vector Models}

\subsection{Regression Trees}



%--------------------------------------
\clearpage
\section{Timeseries Data}

\subsection{Data Concerns}

\subsection{Differencing}

\subsection{Identifying AR;MA - ACF, PACF}

\subsection{Unit Roots}

\subsection{ARIMA}

\subsection{VAR}

\subsubsection{Example VAR}

<<dataload, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE, prompt=TRUE >>=
# devtools::install_github("hadley/haven")
require(haven)
df <- read_dta('C:/Users/Shing/Documents/GodGap/TS Final/anes_timeseries_cdf.dta')
library(ggplot2)
library(ggthemes)
@

\begin{center}
<<desc, warning=FALSE, message=FALSE, out.width='6in', out.height='3.7in', echo=FALSE, include=TRUE>>=
totals <- aggregate(df$VCF0130~df$VCF0004, FUN = NROW)

# Every Week:
n <- aggregate(df$VCF0130==1~df$VCF0004, FUN = sum)
list <- as.vector(n[,2]/totals[,2])
# list*100 # percentage
week <- list*100

# Almost every week:
n <- aggregate(df$VCF0130==2~df$VCF0004, FUN = sum)
list <- as.vector(n[,2]/totals[,2])
# list*100 # percentage
aweek <- list*100

# once or twice a month
n <- aggregate(df$VCF0130==3~df$VCF0004, FUN = sum)
list <- as.vector(n[,2]/totals[,2])
# list*100 # percentage
month <- list*100

# Few times a year
n <- aggregate(df$VCF0130==4~df$VCF0004, FUN = sum)
list <- as.vector(n[,2]/totals[,2])
# list*100 # percentage
ayear <- list*100

# never
n <- aggregate(df$VCF0130==5~df$VCF0004, FUN = sum)
list <- as.vector(n[,2]/totals[,2])
# list*100 # percentage
never <- list*100

year <- n[,1]
year <- as.vector(year)

ggplot(as.data.frame(year), aes(year)) +
     geom_line(aes(y = week), color="black", size=1.3) +
     geom_point(aes(y= week))+
     geom_line(aes(y= ayear), color="burlywood4", size=1.3)+
     geom_point(aes(y= ayear), color="burlywood4")+
     geom_line(aes(y = month), color="tomato", size=1.3)+
     geom_point(aes(y = month), color="tomato")+
     geom_line(aes(y = never), color="orange", size=1.3)+
     geom_point(aes(y = never), color="orange")+
     theme_fivethirtyeight()+
     ggtitle("1970-2012 [VCF0130]\nReligious Attendence (%)")+
     annotate("text", x = 1977, y = 15, label = "Once/Twice A Month", color = "tomato")+
     annotate("text", x = 1991, y = 34.5, label = "Never", color = "orange")+
     annotate("text", x = 1980, y = 23, label = "Every Week")+
     annotate("text", x = 1980, y = 31, label = "Once/Twice A Year", color = "burlywood4")+
     annotate("text", x = 1970, y = 38, label = "**", fontface =2)+
     annotate("text", x = 1989.5, y = 22, label = "*", fontface =2)+
     labs(caption="Source: ANES Cumulative Data File, @KyleDavisOrg
          *In 1990, filter question was added, if R indicated never attending it was included in code here
          **In 1970 'Every Week' was including 'Almost Every Week' ")

@
\end{center}

Using the ANES data, we see some survey changes over time on how we measured religious attendence. To narrow down, I will be analyizing only those respondents that reported weekly attendence. Further, I will be looking at those who reported being ``strongly liberal'' (conservative) including leaning liberal (conservative). Since the latter question was only asked from 1972 on, I will subset the weekly church attendence from 1972 to 2012. This has an unexpected benefit of removing the 1970 survey change on weekly church attendence.

\begin{center}
<<descriptiveplot, warning=FALSE, message=FALSE, out.width='6in', out.height='4in', echo=FALSE, include=TRUE>>=

# totals for denominator
totalid  <- aggregate(df$VCF0804~df$VCF0004, FUN = NROW)


lib <- aggregate(df$VCF0804==1~df$VCF0004, FUN = sum)
listid <- as.vector(lib[,2]/totalid[,2])
# listid*100 # percentage
liberal <- listid*100

con <- aggregate(df$VCF0804==3~df$VCF0004, FUN = sum)
listcon <- as.vector(con[,2]/totalid[,2])
# listcon*100 # percentage
cons <- listcon*100


# getting year total
totalrel <- aggregate(df$VCF0130~df$VCF0004, FUN = NROW)
totalrel <- subset(totalrel , totalrel$`df$VCF0004` >= 1972) # subset to get years to line up


wk <- aggregate(df$VCF0130==1~df$VCF0004, FUN = sum)
wk <- subset(wk , wk$`df$VCF0004` >= 1972) # subset to get years to line up
listrel <- as.vector(wk[,2]/totalrel[,2])
# listrel*100 # percentage
week <- listrel*100

libweek <- aggregate(liberal~week, FUN = sum)
consweek <- aggregate(cons~week, FUN = sum)

year <- totalrel[,1]
year <- as.vector(year)

ggplot(as.data.frame(year), aes(year)) +
     geom_line(aes(y = consweek[,2]), color="tomato", size=1.3)+
     geom_point(aes(y = consweek[,2]), color="tomato")+
     geom_line(aes(y = libweek[,2]), color="cornflowerblue", size=1.3)+
     geom_point(aes(y = libweek[,2]), color="cornflowerblue")+
     theme_fivethirtyeight()+
     ggtitle("1972-2012 [VCF0130:VCF0804]\nWeekly Religious Attendence by Partisanship (%)")+
     labs(caption="Source: ANES Cumulative Data File, @KyleDavisOrg")

@
\end{center}

Using this data, I wish to formally test that the two series are not granger-causal of one another and that the two trends are not deterministic of the other. It is my hypothesis that the two series do not rely on one another, but larger economic and sociopolitical influences are at play. Following testing this hypothesis, I will end this assignment discussing future directions.

\textbf{Testing for Optimal Lag Order for VAR}

Before I begin testing for optimal lag length for the model, I test for the presence of unit roots. The following chunk of R code goes through setting up the data and conducting unit root tests. It reveals that in every unit root test I conduct, there is evidence against the presence of unit roots and nonstationarity.\footnote{In setting up the time series in R, I had to set the end date back, to get the right frequency to line up with skipped year surveys. Regardless, all data is still present, the year index label for 2012 is, however, masked. This should not change any results, I checked the plots between the full ANES dat and the time series object reported here for any discrepencies, finding none.}

<<unitroot, warning=FALSE, message=FALSE, prompt=TRUE, eval=TRUE, include=TRUE, echo=TRUE, results='hide'>>=
libweekTS <- ts(data = libweek[,2],
                start = head(year, n = 1),
                end = 2008, # 2008 end here because ANES aregular surveys
                frequency = .5)
# plot(libweekTS) # Checking

consweekTS <- ts(data = consweek[,2],
                start = head(year, n = 1),
                end = 2008,
                frequency = .5)
# plot(consweekTS) # Checking

## Unit root testing:
library(tseries)
 adf.test(libweekTS)
 adf.test(consweekTS)
# Reject the null of non - stationarity and a unit root

 kpss.test(libweekTS)
 kpss.test(consweekTS)
# Reject the null of stationarity around a deterministic trend

 PP.test(libweekTS)
 PP.test(consweekTS)
# Reject the null that the series is integrated of order 1 (unit root)
@

Following these tests I feel confident that there are no unit roots. Moving to testing the lag-order of the model is realitively easy to do in R, and give us the lag order based upon different information critera such as the AIC, HQ, SC, and FPE. Results of these tests all indicate best lag-length of four. \index{Augmented Dicky Fuller} \index{KPSS Test}

<<lag, warning=FALSE, message=FALSE, prompt=TRUE>>=
# Ideal lag order:
varData <- data.frame(libweekTS, consweekTS)

library(vars)

lagselection <- VARselect(varData, lag.max = 10, type = "none")
lagselection$selection
@

\textbf{Granger--Causality Test} \index{Granger Causality}

A time series is said to be Granger-causal to an outcome if it (through lagged values) can provide statistically significant information about the outcome. This relationship is based upon two requirements: the cause happens prior to its effect (lagged values), and the cause provides unique information about the future values of its effect. These assumptions are in some-way built into nearly every cause inference test. \index{Vector Auto Regression}

More specifically, a variable $X$ is Granger-causal to some $Y$ if the prediction of $Y_t$ based upon all of its past lagged values $(Y_{t-1}, Y_{t-2}, ...)$ and on lagged values of $X$ does better than just $Y$ based solely on it's past values alone. Put another way, granger-causality compares two predictive models (``worlds''), one where $X$ exists or where $Y$ is soley predictive upon itself. If the model which includes $X$ does better than the $Y$ being predictive of itself, $X$ is said to be Granger-causal of $Y$.

<<gcause, warning=FALSE, message=FALSE, prompt=TRUE>>=
var1 <- VAR(varData, p=4, type="const") # Indicating lag length 4 here.
 # summary(var1) VAR results, including a 4-lag model results.
 # plot(var1) # can easily check ACF and PACF here

 granger1 <- causality(var1, cause="consweekTS")
 granger1$Granger$method
 granger1$Granger$p.value

 granger2 <- causality(var1, cause="libweekTS")
 granger2$Granger$method
 granger2$Granger$p.value
@




This hypothesis test takes as the null that $X$ is not a Granger-cause of $Y$ , and as such, a p-value below some $\alpha$ level would lead one to reject the null hypothesis. These tests lead me to confirm that neither series is Granger-causal of the other. Placed back in theory, neither liberals nor conservatives follow each others trends in weekly church attendence -- indicating other sociological, political, or economic forces to be at play.

A sign-and-significance reading of the Granger causality results conclude that, with 12 degrees of freedom, neither variation of the Granger-causaility test pass statistical significance to declare Granger-causality. The liberal timeseries onto the conservative series holds a p-value of .96, and the conservative series on the liberal holds a p-value of .34. Neither pass conventional .05 thresholds because the series better explains itself than other explains it with respect to the VAR with lag length four.


\textbf{Innovation Accounting} \index{Innovation Accounting}


Innovation accounting can tell us how a VAR model responds to individual and orthogonalized shocks to each of the variables. This necceciates an assumption that our variable orderings matter as these shocks transition through our structure. However, orthogonalizing the equation allows for us to see the marginal effects, if we didn't do this we would allow the variable to increase its ripples throught the structure without controlling for confounds that could occur.

In this particular case I am not sure if the ordering of variables really matters much, I think respondents from  year-to-year don't really consider their parties past responses to church identification all that much. The figure below kind-of shows this, a moving around 0, insignificant on starndard thresholds.

Finally we move to looking at the forecast error variance decomposition, which allows for examining the effect of a particular variable to the future forecast error variance of another variable. To examine this, see the plots below the innovation accounting. These reveal that not one particular variable is contributing to forecast uncertainty. It is notable, however, that the FEVD did detect some variance explained by liberal church attendence on liberal church attendence in the first two horizons.


\begin{center}
<<inov, warning=FALSE, message=FALSE, prompt=TRUE, out.height='4.5in'>>=

# Innovation accounting:
 irf1 <- irf(var1, impulse = "consweekTS", response = "libweekTS")
 plot(irf1)
 irf2 <- irf(var1, impulse = "libweekTS", response = "consweekTS")
 plot(irf2)

# Forecast Error Variance Decomposition
 fevd1 <- fevd(var1, n.ahead = 5)
 plot(fevd1)

@
\end{center}


\textbf{Summary, Concluding Thoughts}

Overall, the purpose of this assignment was to test the relationship between two series and report how they may relate or not. I chose to test a fundemental underlying assumption of the ``god gap'' literature, for if these two series did influence one-another then all past examinations would need to take in to account this. However I have found little evidence of this occuring. This makes sense, survey responses of one partisan on church attendence are likely not to consider the other partisan and their historical activites. Rather, I think larger socioeconomic, historical, and geographical variables matter more at moving partisians to church at different levels over time. VAR too can be used to study these variables in realtion to the others.

While VAR allows for flexibility in helping determine the data generating process, proper treatment of the error term through lag-length detection, and easy transition to error correction models and OLS it is not without its disadvantages. If a series is nonstationary, statistical testing can become problematic.\footnote{Box-Steffensmeier, Janet M., John R. Freeman, Matthew P. Hitt, and Jon CW Pevehouse. Time series analysis for the social sciences. \textit{Cambridge University Press}, 2014.} If the series is cointegrated differencing may not solve these problems. Finally, as I mentioned before, I think a lot of different variables influence the widening and contraction of the god gap, however including all of them for all of their past lags may easily overfit the model if proper percautions are not used.\footnote{Many thanks to Benjamin Campbell for his help with the coding and theory behind these methods.}



%--------------------------------------
\clearpage
\section{Network Analysis}

\subsection{Network Theory}

\subsection{Network Data, Nodes, and Edges}

\subsection{Summary Statistics of Networks}

\subsection{Centrality}

Measuring the centrality of a network is indicating which node(s) are most central, or critical, to the network remaning the shape it is in. A central node, for example, can be the person inbetween groups, or just a well connected node in general. There are many ways to measure and detect network centrality, some of which I've studied are listed below. Later in this section we will compare each of these finding that they typically correlate with one another (good!). Cannon advice is to report all of the relevant measures of centrality rather than spending too much time choosing any particular one.

\textit{Borgatti (2005):}
\begin{longtable}{m{2.8cm}  m{4cm} m{5.7cm} m{4.7cm}  }
\hline\noalign{\smallskip}
\textbf{Name} & \textbf{Definition} & \textbf{Properties} & \textbf{Notes} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Betweenness (Freeman, 1979) & $\mathlarger{ \Sigma_i \Sigma_j \frac{g_{ikj}}{g_{ij}}, ~~ i \neq j \neq k }$ & Assumes traffic is indivisible, traffic takes known-shortest paths,   & Likely not a good index for information flow or ``gossip'' \\
Eigenvector centrality (Bonacich, 1972) & $\lambda v = A v$ & Traffic moves along unrestricted ``walks.'' Traffic can effect subsequent nodes simultaneously. Measures long term and indirect risk.   & Related to other measures on sequentially influencing nodes (``influence-type process'').\\
Degree centrality (Freeman, 1979) & Limiting probabilities of nodes by their degree. (Markov Process).    & Measures immediate risk. Assumes an infinitely long random walk (a dollar traveling hands). \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{longtable}


But how do we actually do this? Let's start with reading in the data, and setting everything up. I'll save space by not commenting this inital code, just know that it is creating an edgelist matrix for us to generate network objects. This data is from Google's BigQuery - Reddit data in October and November (2017) for the r/asksocialscientists page. This contains those who post and those who comment. \index{recoding data}
<<data, warning=FALSE, message=FALSE, prompt=TRUE>>=
# Initial Packages:
library(statnet)
library(dplyr)
library(igraph)

# Let's read in, add on top of one/ another the Reddit EdgeList data:
##   October:
# read in the posts data
posts <- read.csv('file:///C:/Users/Shing/Documents/Previous Course Info/Networks/Data/2017_10_AskSocialScience_posts.csv')
# read in the comments data
comments <- read.csv('file:///C:/Users/Shing/Documents/Previous Course Info/Networks/Data/2017_10_AskSocialScience_comments.csv')
comments$id2 <- paste("t1_", comments$id, sep="")
posts$id2 <- paste("t3_", posts$id, sep="")
for_el <- data.frame(
     author = c(as.character(posts$author), as.character(comments$author)),
     id = c(as.character(posts$id2), as.character(comments$id2)),
     parent_id = c(rep(NA, dim(posts)[1]), as.character(comments$parent_id))
)

for_el <- for_el[for_el$author!="[deleted]",]
el1 <- inner_join(for_el, for_el, c("parent_id" = "id"))
el <- data.frame(auth1 = el1$author.x, auth2 = el1$author.y)
el <- as.matrix(el)

#---------------------------------------------------------------------------
##   Nov:
posts2 <- read.csv('file:///C:/Users/Shing/Documents/Previous Course Info/Networks/Data/2017_11_AskSocialScience_posts.csv')
comments2 <- read.csv('file:///C:/Users/Shing/Documents/Previous Course Info/Networks/Data/2017_11_AskSocialScience_comments.csv')
comments2$id2 <- paste("t1_", comments2$id, sep="")
posts2$id2 <- paste("t3_", posts2$id, sep="")

for_el2 <- data.frame(
     author = c(as.character(posts2$author), as.character(comments2$author)),
     id = c(as.character(posts2$id2), as.character(comments2$id2)),
     parent_id = c(rep(NA, dim(posts2)[1]), as.character(comments2$parent_id))
)
for_el2 <- for_el2[for_el2$author!="[deleted]",]
el12 <- inner_join(for_el2, for_el2, c("parent_id" = "id"))
el2 <- data.frame(auth1 = el12$author.x, auth2 = el12$author.y)
el2 <- as.matrix(el2)

@

Now that we have the data read in and cleaned (mostly tracking the id numbers), I'll combine the two months and plot the network object that we'll be using for the rest of this section.

\begin{center}
<<starting, warning=FALSE, message=FALSE, prompt=TRUE, out.width='6in'>>=
# Stack one edge list on top of the other:
edge.list <- rbind(el, el2)
head(edge.list)

graph.object <- graph_from_edgelist(edge.list)
summary(graph.object) # Basic statistics seem okay

# Grabbing the largest component:
giant.component <- function(graph) {
     cl <- clusters(graph)
     induced.subgraph(graph, which(cl$membership == which.max(cl$csize)))
}

l.graph.object <- giant.component(graph.object)

# Let's graph it
par(mfrow=c(1,2))
plot.igraph(graph.object,
            vertex.color="gray15",
            vertex.size=2.7,
            vertex.label=NA,
            vertex.shape="circle",
            edge.arrow.size=0.4,
            edge.arrow.width=.6,
            edge.width=1.5)
title("\n October:November; 2017",
      cex.main=1, col.main="gray10")

plot.igraph(l.graph.object,
            vertex.color="gray15",
            vertex.size=2.7,
            vertex.label=NA,
            vertex.shape="circle",
            edge.arrow.size=0.4,
            edge.arrow.width=.6,
            edge.width=1.5)
title("\n Largest Component",
      cex.main=1, col.main="gray10")
par(mfrow=c(1,1))

@
\end{center}

We can examine some measures of centrality from this network (using the full network instead of largest component). I'll calculate the degree, the inward degree, and outward degree. Then, I find the summary statistics and plot all of these in one concise plot.

\begin{center}
<<degree, warning=FALSE, message=FALSE, prompt=TRUE, out.width='6in'>>=
# For this assignment, we are only interested in large component:
graph.object <- l.graph.object

# --
degree <- degree(graph.object)
in.degree <- degree(graph.object, mode='in')
out.degree <- degree(graph.object, mode='out')

# Summary Statistics:
summary(out.degree)
summary(in.degree)
summary(degree)

sd(out.degree)
sd(in.degree)
sd(degree)

# Grammar of Graphics:
library(ggplot2)
library(gridExtra)
library(grid)

p1 <- qplot(out.degree)+
     theme_bw()+
     xlab("Outward Degree")+
     ylab("N")+
     geom_vline(xintercept=1.859, linetype="dashed")+
     annotate("text", x = 50, y = 400, label = "sd = 3.30")

p2 <- qplot(in.degree)+
     theme_bw()+
     xlab("Inward Degree")+
     ylab("N")+
     geom_vline(xintercept=1.859, linetype="dashed")+
     annotate("text", x = 16, y = 213, label = "sd = 2.23")

p3 <- qplot(degree)+
     theme_bw()+
     xlab("Degree")+
     ylab("N")+
     geom_vline(xintercept=3.718, linetype="dashed")+
     annotate("text", x = 61, y = 214, label = "sd = 4.76")

grid.arrange(
     p1, p2, p3,
     # Graphing Matrix:
     widths = c(.5, .9 ,2, 2, .9, .5),
     layout_matrix = rbind(c(NA, 1, 1, 2, 2, NA),
                           c(NA, NA, 3, 3, NA, NA)),
     top = "Network Degree Distributions;
          \n Oct : Nov'r/AskSocialScientists' Reddit Data.",
     # Any citations:
     bottom = textGrob(
          "Mean Reported by Dashed Line.
          \n All Medians = 1 except 'Degree' which = 2 ",
          gp = gpar(fontface = 3, fontsize = 8),
          hjust = 1,
          x = 1
     )
)

@
\end{center}


The next part was difficult, and probably could be done better. I looked at a list of the ordered centrality measures, then created an edge-graph and looked at those lists similarly. We can see from the lists that some names pop up more often then others (MoralMidgetry, apaniyam, Atriox998), but there has to be some code to get the ``max'' or to highlight those with the largest degree more clearly. How might we code that? \index{ggplot}

<<listsgalor, warning=FALSE, message=FALSE, prompt=TRUE>>=

### Centrality measures:
lg_graph.object <- line.graph(graph.object)

## closeness centrality
c.graph.object <- closeness(graph.object)
summary(c.graph.object)
E(graph.object)[order(c.graph.object, decreasing=T)[1:10]]

c.lg.graph.object <- closeness(lg_graph.object)
E(graph.object)[order(c.lg.graph.object, decreasing=T)[1:10]]


## betweenness centrality
b.graph.object <- betweenness(graph.object)
summary(b.graph.object)
E(graph.object)[order(b.graph.object, decreasing=T)[1:10]]

b.lg.graph.object <- betweenness(lg_graph.object)
E(graph.object)[order(b.lg.graph.object, decreasing=T)[1:10]]


## eigenvector centrality
e.graph.object <- evcent(graph.object)
summary(e.graph.object$vector)
E(graph.object)[order(e.graph.object$vector, decreasing=T)[1:10]]

evc_lg_graph.object <- evcent(lg_graph.object)
E(graph.object)[order(evc_lg_graph.object$vector, decreasing=T)[1:10]]


@


At least from above we know where to start looking for central nodes within our data. Following this, I compare each measure of centrality by generating a matrix of correlation values, then plot it.

\begin{center}
<<comp, message=FALSE, warning=FALSE, out.width='6in', prompt=TRUE>>=

# Let's whip-up a matrix of just our values, look at the correlation between them.
vec.degree <- as.vector(degree)
vec.in.degree <- as.vector(in.degree)
vec.out.degree <- as.vector(out.degree)
vec.c.graph.object <- as.vector(c.graph.object)
vec.b.graph.object <- as.vector(b.graph.object)
vec.e.graph.object <- as.vector(e.graph.object$vector)


Cor.Object <- matrix(c(vec.degree, vec.in.degree, vec.out.degree,
                       vec.c.graph.object, vec.b.graph.object, vec.e.graph.object),
                      ncol=6)

colnames(Cor.Object) <- c("Degree", "In Degree", "Out Degree",
                         "Closeness", "Betweeness", "Eigenvector")

cor(Cor.Object)

# Trying to think of cool ways to plot this:
library(corrplot)
x <- cor(Cor.Object)
corrplot(x, type="upper", order="hclust")

@
\end{center}


I find it easier to think about correlation by looking at the plot (albiet a very rough plot). The two ``darkest'' or most correlated measures was the in-degree with degree, and the degree with the out-degree. This makes sense because the degree is encapsulating both of those measures. All measures are positively correlated, except the eigenvector centrality measure compared to closeness and betweeness. This is likely because of the different assumptions and properties each of these measures have. For example, eigenvector centrality Trafic moves along unrestricted walks, Trafic can effect subsequent nodes simultaneously, and eigenvector centrality measures long-term and indirect risks. Other measures may not have these properties, so it is probably best that they are not too closely correlated.


%--------------------------------------
\subsubsection{Data Viz}

Perhpas now is a good opportunity to talk about data vizualisation. Using the data from the previous section on centrality:
In this I will be using the Reddit data from "r/AskSocialScientists" from October and November. These will be ran seperately, and I will look into their largest components. Then, I will plot the degrees in two different ways.

<<dataforviz, prompt=TRUE, warning=FALSE, message=FALSE>>=
# Using the data set up from the previous section:
##    Oct:
library(igraph)
g1 <- graph_from_edgelist(el)
summary(g1) #

#---------------------------------------------------------------------------
##   Nov:
g12 <- graph_from_edgelist(el2)
summary(g12) #

@

\begin{center}
<<plot1, prompt=TRUE, out.width='5in'>>=
set.seed(8675309)  # 1981 throwback seed

par(mfrow=c(1,2))
plot.igraph(g1,
            vertex.color="gray15",
            vertex.size=2.7,
            vertex.label=NA,
            vertex.shape="circle",
            edge.arrow.size=0.4,
            edge.arrow.width=.6,
            edge.width=1.5)
title("\n October; 2017",
      cex.main=1, col.main="gray10")

plot.igraph(g12,
            vertex.color="gray15",
            vertex.size=2.7,
            vertex.label=NA,
            vertex.shape="circle",
            edge.arrow.size=0.4,
            edge.arrow.width=.6,
            edge.width=1.5)
title("\n November; 2017",
      cex.main=1, col.main="gray10")
par(mfrow=c(1,1)) # basic reset

@
\end{center}

Above we have both of our complete networks for both months. In these graphs we simply have users (nodes) and if they have commented on each other's posts (edges). This may include some information that is less interesting, like users that have no comments. Below we will take the two largest components of each network and plot it.

\begin{center}
<<plot2, prompt=TRUE, warning=FALSE, out.width='5in'>>=

giant.component <- function(graph) {
     cl <- clusters(graph)
     induced.subgraph(graph, which(cl$membership == which.max(cl$csize)))
}

g2 <- giant.component(g1)
g22 <- giant.component(g12)


par(mfrow=c(1,2))
plot.igraph(g2,
            vertex.color="gray15",
            vertex.size=2.7,
            vertex.label=NA,
            vertex.shape="circle",
            edge.arrow.size=0.4,
            edge.arrow.width=.6,
            edge.width=1.5)
title("\n October; 2017",
      cex.main=1, col.main="gray10")

plot.igraph(g22,
            vertex.color="gray15",
            vertex.size=2.7,
            vertex.label=NA,
            vertex.shape="circle",
            edge.arrow.size=0.4,
            edge.arrow.width=.6,
            edge.width=1.5)
title("\n November; 2017",
      cex.main=1, col.main="gray10")

@
\end{center}

For the above, I have chosen to keep plots simple and not too complex to overcomplicate interpretation. In a paper these graphs would likely print and be easily read. Some things that I may consider chaning is making the network bigger, and to have directionality more clear.\\

This is, of course, not the only information we can gain from a network. Below is a plot of the network degrees. A network "degree" is the number of connections each node has - a useful statistic. The following graphs intended purpose is to report the mean, the nodes, and edges in addition to the distribution of degrees in a network.

\begin{center}
<<plot3, prompt=TRUE, out.width='5in', warning=FALSE, message=FALSE>>=

Degree <- degree(g2)
mean(Degree)

library(ggplot2)
qplot(Degree)+
     theme_bw()+
     ylab("N")+
     xlab("Degrees")+
     #ggtitle("Degrees in Large Component of Network 1")+
     ggtitle(substitute(
          paste( "Large Component Degrees, with: ", N[V], "=", 231,
                 " and ", N[E], "=", 404)))+
     annotate("text", x = 35.4, y = 3.7, label = "35")+
     geom_vline(xintercept=mean(Degree), linetype="dashed")+
     annotate("text", x = 4.3, y = 71, label = round(mean(Degree),3), angle=-90)

@
\end{center}

We might be interested in the same network statistic but reported with the help of feline friends. Certainly journals today would not accept this, but perhaps they will someday when the job market slows down. Below is a ``CatterPlot'' of the degrees in our network, cats are in two colors to better show distinction. Note the clowder of cats in the lower-degree levels and the one cat in the top left being the most ``social'' in our Reddit data. One advantage to a plot like this could be how it attracts the reader and adds a element of humor; however, the plot could be easily distracting and obnoxious to use almost anywhere. \index{ggplot}

\begin{center}
<<what, prompt=TRUE, warning=FALSE, message=FALSE, out.width='5in'>>=
Degree <- degree(g2)
x <- c(1:231)

## Creating a catterplot:
# library(devtools)
# install_github("Gibbsdavidl/CatterPlots")
library(CatterPlots)

meow <- multicat(xs=x, Degree,
                 cat=c(1,2,3,4,5,6,7,8,9,10),
                 catcolor=list('#0495EE', '#FF0000'),
                 canvas=c(-0.1,1.1, -0.1, 1.1),
                 xlab="Index", ylab="Degree Per Node (Cat) in Network",
                 main="A Catterplot of Network Degrees")
@
\end{center}



%--------------------------------------
\clearpage
\section{Field Research Design: The Qualitative Alternative}

\subsection{Qualitative Value}

\subsection{Advice, Tools, Resources}

\subsection{How to Use Qualitative Evidence}




%--------------------------------------

\clearpage
\section{Content Analysis}

\subsection{The Protocol}

\subsection{The Coding Sheet}

\subsection{Reliability Measurements}

\subsection{Analysis}




%--------------------------------------------------------------------
\clearpage
\section{Formal Modeling}

\subsection{How can Formal Modeling Help?} \index{DAG}

The late statistician George Box once said ``All models are wrong, but some are useful.'' Formal Modeling (which I'll sometimes call game theory synonymously) can be used to spell out the the particular assumptions leading to an emperical test. Instead of writing long-hand the history, studies, and assumptions that go alongside our theory we explicity state relevent ``variables'' in math notation leading right up to a logical emperical test. This has many obvious benefits, chiefly it diciplines one to be explicit with their assumptions (similar, but more involved than DAG plots from the causal inference chapter). Also, this allows for clear objections and revisions to be made by future scholarship. Indeed many models, such as Downs' ``Median Voter Theorem'' has been revised multiple times given the clear writing from the original mathematical formal modeling.

Of course Downs and others who use formal modeling are very frequently ``wrong,'' but the purpose of formal modeling is to do things more systematically and with more academic rigor than prior. Works of this nature become foundational, having long shelf-lifes, and become solid building blocks for future scholarship to sculpt.


\subsection{Components of Game Theory}

\hfill \\

\noindent \textbf{Utility, First Best, Nash Equilibrium, Comparative Static}

\index{utility}
\noindent To begin, utility is one's preferences for one thing over another. For example, prefering guns to butter:

$$ u_i(\textit{Guns}) = 10 ~~~~ u_i(\textit{Butter}) = 1 $$\\

\noindent Which, note, is the same as this \textit{utility function}:

$$ u_i(\textit{Guns}) = 8,675,309 ~~~~ u_i(\textit{Butter}) = 1,337 $$\\

One's \textit{expected utility} is the mathematical calculation of one's probability of each event happening multiplied by each utility per outcome.\footnote{pg. 339 of Bueno de Mesquita's ``Political Economy for Public Policy has a great example of this. }

\index{First Best} \index{Nash Equilibrium}
\noindent Considering the standard 2x2 matrix format:

  \begin{table}[h!]
  \centering
    \setlength{\extrarowheight}{2pt}
    \begin{tabular}{cc|c|c|}
      & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Player $Y$}\\
      & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$A$}  & \multicolumn{1}{c}{$B$} \\\cline{3-4}
      {Player $X$}  & $A$ & $(x,y)$ & $(x,y)$ \\\cline{3-4} % \multirow{2}* was in front of {player}
      & $B$ & $(x,y)$ & $(x,y)$ \\\cline{3-4}
    \end{tabular}
  \end{table}

The \textit{First Best} is the bolded sections given the best strategic decisions per potential choice by the other player.


  \begin{table}[h!] % here!
  \centering        % I guess this is the only way to center the table?
    \setlength{\extrarowheight}{2pt}
    \begin{tabular}{cc|c|c|}
      & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Country $Y$}\\
      & \multicolumn{1}{c}{} & \multicolumn{1}{c}{Don't Arm}  & \multicolumn{1}{c}{Arm} \\\cline{3-4}
      {Country $X$}  & Don't Arm & $\textbf{4, 4} $ & 0,\textbf{3} \\\cline{3-4}
      & Arm & \textbf{3},0 & 1,1 \\\cline{3-4}
    \end{tabular}
  \end{table}

The \textit{Nash Equilibrium} is the ``first best of the first bests'' or where all players reach an equilibrium around one set of strategies -- even though they may not have the highest pay-off or reward for a particular player. Note that the process here for solving a 3x3 to NxN matrix is similar to the standard 2x2. Also the Nash Eqilibrium(a) can be solved algebraicly as well; graphically it is the intersection of two lines which represent the strategic continum a player has. A Nash Eqilibrium(a) can be notated by any given variable: $\theta^*$.

\index{subgame perfect Nash Equilibrium}
A variation of the Nash Equilibrium is the \textit{subgame perfect Nash Equilibrium}, or the regular Nash Eqilibria per subgame of a longer, more dynamic, game.

\index{comparative static}
Another term, the \textit{comparative static} is the component of a model that connects our theory to our emperics through game-theoretic reasoning. Typically the comparative static is deployed in papers to test hypotheses, arrive at novel findings, argue for an expansion of a particular model or theory, or just to describe a phenomenon with more precision than before.


\subsection{Popular Games for Neat Examples}



\subsection{Games with Political Relevance}








\subsection{Structuring Your Own Formal Model}








%-----------------------------------------------------------------------
\clearpage
\addcontentsline{toc}{section}{Further Reading:}


\begingroup
	\nocite{*}

	%\setlength{\bibsep}{12pt}

	%\setstretch{2}

	\bibliography{GLM}

	\bibliographystyle{apsr}

\endgroup


%\clearpage
%\bibliographystyle{plain}
%\bibliography{GLM.bib}
%\printbibliography




%-----------------------------------------------------------------------
\clearpage
\addcontentsline{toc}{section}{Index}
\printindex











\end{flushleft}
\end{document}